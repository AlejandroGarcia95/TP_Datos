{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lectura del archivo .csv de training\n",
    "import pyspark_csv as pycsv\n",
    "sc.addPyFile('pyspark_csv.py')\n",
    "plaintext_rdd = sc.textFile('train.csv')\n",
    "dataframe = pycsv.csvToDataFrame(sqlCtx, plaintext_rdd, parseDate=False)\n",
    "\n",
    "data = dataframe.rdd\n",
    "data = data.sample(False, 0.001)\n",
    "#data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # Para eliminar tags html\n",
    "import re # Expresiones regulares para eliminar puntuacion\n",
    "from nltk.corpus import stopwords # Stopwords para eliminar palabras comunes\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "def aplicarStemming(x):\n",
    "    words = x.split()\n",
    "    st = LancasterStemmer()\n",
    "    new_words = []\n",
    "    for w in words:\n",
    "        new_words.append(st.stem(w))\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "def borrarPalabrasComunes(x):\n",
    "    words = x.split()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    stop_words.remove(\"not\")\n",
    "    new_words = []\n",
    "    for w in words:\n",
    "        if(not w in stop_words):\n",
    "            new_words.append(w)\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "def borrarSimbolos(x):\n",
    "    aBorrar = \",@#$-.():[]!?\"\n",
    "    for c in aBorrar:\n",
    "        x = x.replace(c, \" \")\n",
    "    return x\n",
    "\n",
    "def considerarEmoticonesPuntuacion(x):\n",
    "    # Lista de caritas felices\n",
    "    caras_felices = [\":)\", \"(:\", \"[:\", \":]\", \"c:\", \"=)\", \"=]\", \"(=\", \"[=\", \"c=\",\n",
    "                    \"=D\", \":D\", \";)\", \"(;\", \";D\"]\n",
    "    for emoji in caras_felices:\n",
    "        x = x.replace(emoji , \"SMILING_FACE\")\n",
    "    # Lista de caritas tristes\n",
    "    caras_tristes = [\":(\", \":[\", \"):\", \"]:\", \":c\", \"=(\", \"=[\", \"]=\", \"=c\", \"D=\", \n",
    "                    \"D:\", \";(\", \");\", \"D;\", ]\n",
    "    for emoji in caras_tristes:\n",
    "        x = x.replace(emoji, \"SAD_FACE\")\n",
    "    # Lista de caritas sorprendidas\n",
    "    caras_sorpr = [\":0\", \":o\", \"0:\", \"o:\", \"=o\", \"0=\"]\n",
    "    for emoji in caras_sorpr:\n",
    "        x = x.replace(emoji, \"SURPRISED_FACE\") \n",
    "    # Puntuación (signos ! y ?)\n",
    "    x = x.replace(\"!!!\", \" ADMIRx3\")\n",
    "    x = x.replace(\"!!\", \" ADMIRx2\")\n",
    "    x = x.replace(\"???\", \" QUESx3\")\n",
    "    x = x.replace(\"??\", \" QUESx2\")\n",
    "    x = x.replace(\"?!\", \" ADM_QUES\")\n",
    "    x = x.replace(\"!?\", \" ADM_QUES\")\n",
    "    x = x.replace(\"!\", \" ADMIRx1\")\n",
    "    x = x.replace(\"?\", \" QUESx1\")\n",
    "    return x\n",
    "\n",
    "# Función encargada de realizar un pre-procesamiento de los textos de las reviews\n",
    "# según lo considerado por nuestro diseño del TP. Para ello, se recibe el set de\n",
    "# entrenamiento como un RDD de reviews, que son tuplas (texto, puntaje).\n",
    "# Las distintas acciones que la función realiza sobre el texto de las reviews\n",
    "# dependen de los flags de procesamiento recibidos en flagsP (como lista).\n",
    "# A continuación la lista de acciones controlada por cada flag de flagsP:\n",
    "# flagsP[0] controla la eliminación de palabras comunes (\"a\", \"the\", \"of\", etc.)\n",
    "# fragsP[1] elimina las palabras de frecuencia menor a *un número*\n",
    "# flagsP[2] activa el uso de stemming sobre las palabras de la review\n",
    "# flagsP[3] activa el reconocimiento de emoticones y puntuaciones ?,!\n",
    "# continuar...\n",
    "# Acciones que el pre-procesador de reviews hace siempre:\n",
    "# - Eliminar tags html\n",
    "# - Convertir todo a minúsculas\n",
    "# - Eliminar los siguientes símbolos: \",\" \"@\" \"#\" \"$\" \"-\" \".\" \"(\" \")\" \":\" \"]\" \"[\"\n",
    "# (En el caso de considerar emoticones o puntuación no lo hace hasta después de\n",
    "# detectar todos los emoticones o símbolos deseados correspondientes)\n",
    "def preprocesar_reviews(elSet, flagsP):\n",
    "    nuevoSet = elSet.map(lambda x: (BeautifulSoup(x[0], \"lxml\").getText(), x[1]) )\n",
    "    nuevoSet = nuevoSet = nuevoSet.map(lambda x: (x[0].lower(), x[1]))\n",
    "    \n",
    "    if(flagsP[0]):\n",
    "        nuevoSet = nuevoSet.map(lambda x: (borrarPalabrasComunes(x[0]), x[1]))\n",
    "    \n",
    "    if(flagsP[2]):\n",
    "        nuevoSet = nuevoSet.map(lambda x: (aplicarStemming(x[0]), x[1]))\n",
    "    \n",
    "    if(flagsP[2]):\n",
    "        nuevoSet = nuevoSet.map(lambda x: (considerarEmoticonesPuntuacion(x[0]), x[1]))\n",
    "    nuevoSet = nuevoSet = nuevoSet.map(lambda x: (borrarSimbolos(x[0]), x[1]))\n",
    "    \n",
    "    return nuevoSet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Todo este bloque define la realización del k-fold crossed validation.\n",
    "# El método funciona así: recibe un set de entrenamiento y hace sobre el\n",
    "# mismo la técnica de k-fold crossed validation. El formato del set debe\n",
    "# ser un RDD de TUPLAS de la forma: (features, categoria) donde la clave\n",
    "# features puede ser cualquier basura, y categoria es el valor numérico\n",
    "# que se desea predecir (aka el puntaje de cada review). En cada pasada\n",
    "# del k-fold crossed validation, se invocan a las funciones de entrenar\n",
    "# func_entrenar y a las de predicción func_predecir. Éstas dos funciones\n",
    "# deben trabajar de manera global con el/los compresor/es o el SVM. Sus\n",
    "# firmas deben ser las siguientes:\n",
    "# func_entrenar recibe un set de entrenamiento (en el mismo formato que\n",
    "# el set original, como tuplas feature,cat.) y prepara al compresor o SVM\n",
    "# para las predicciones usando ese set.\n",
    "# func_predecir recibe un set a predecir (en el mismo formato de tuplas\n",
    "# feature, cat) y debe devolver OTRO set (también en el mismo formato!)\n",
    "# que correspondan a las predicciones hechas por el SVM o compresores.\n",
    "# Observación importante: como el k-fold crossed validation en sí no\n",
    "# tiene ni idea qué usamos para predecir, todo lo demás ajeno a eso,\n",
    "# incluyendo la selección de hiperparametros, debe hacerse \"por fuera\",\n",
    "# ya sea con un pre-procesamiento de las reviews o en la función de entrenar.\n",
    "\n",
    "def fooCount(x):\n",
    "    global contadora\n",
    "    contadora += 1\n",
    "    return (x, contadora)\n",
    "\n",
    "def calculo_ECM(predSet, valSet):\n",
    "    #cant = predSet.count()\n",
    "    cant = 1\n",
    "    setAux = predSet.union(valSet)\n",
    "   # setAux = setAux.map(lambda x: (np.array_str(x[0]), x[1]) )\n",
    "    setAux = setAux.reduceByKey(lambda x,y: float(x)-float(y)).map(lambda x: x[1]*x[1])\n",
    "    ecm = setAux.reduce(lambda x,y: x+y)\n",
    "    return (ecm/float(cant))\n",
    "\n",
    "def k_fold_crossed_validation(elSet, func_entrenar, func_predecir):\n",
    "    cantParticiones = 8\n",
    "    ecm_acum = 0.0\n",
    "    largoSet = elSet.count()\n",
    "    largoParticion = largoSet / cantParticiones\n",
    "    setauxi = elSet.map(fooCount)\n",
    "    for j in range (1, cantParticiones+1):\n",
    "        # Obtengo el testSet como la particion j-ésima y el trainSet como\n",
    "        # todo el resto del set recibido menos el testSet\n",
    "        #print \"El set:\", elSet.take(2)\n",
    "        testSet = setauxi.filter(lambda x: (x[1] % cantParticiones) == (j-1)).map(lambda x: x[0])\n",
    "        trainSet = setauxi.filter(lambda x: (x[1] % cantParticiones) == (j-1)).map(lambda x: x[0])\n",
    "        #print \"Antes de entrenar:\", trainSet.take(2)\n",
    "        # Entreno contra trainSet\n",
    "        func_entrenar(trainSet)\n",
    "        # Testeo contra testSet\n",
    "        setResultados = func_predecir(testSet)\n",
    "        ecm_acum += calculo_ECM(setResultados, testSet)\n",
    "        print \"ECM acumulado iteracion\", j, \"es:\", ecm_acum\n",
    "    # Obtengo el ECM promedio de la validación\n",
    "    print \"ECM promedio:\", (ecm_acum/float(cantParticiones))\n",
    "    \n",
    "    \n",
    "contadora = 0 # Se usa, no tocar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECM acumulado iteracion 1 es: 559.0\n",
      "ECM acumulado iteracion 2 es: 1165.0\n",
      "ECM acumulado iteracion 3 es: 1840.0\n",
      "ECM acumulado iteracion 4 es: 2513.0\n",
      "ECM acumulado iteracion 5 es: 3107.0\n",
      "ECM acumulado iteracion 6 es: 3805.0\n",
      "ECM acumulado iteracion 7 es: 4350.0\n",
      "ECM acumulado iteracion 8 es: 4921.0\n",
      "ECM promedio: 615.125\n"
     ]
    }
   ],
   "source": [
    "from ppmc import *\n",
    "#from MulticatSVM import *\n",
    "import numpy as np\n",
    "import gc\n",
    "def crear_compresor(x):\n",
    "    aux = CompresorHibrido(5)\n",
    "    aux.entrenar(x)\n",
    "    return aux\n",
    "\n",
    "def combinar_compresor(x, y):\n",
    "    x.combinarTabla(y.verTablas())\n",
    "    return x\n",
    "\n",
    "def entrenar_comp(trainSet):\n",
    "    return\n",
    "    global comp\n",
    "    comp = []\n",
    "    gc.collect()\n",
    "    for i in range(1, 6): # 5 puntajes posibles\n",
    "        rev_act = trainSet.filter(lambda x: x[1] == i).map(lambda x: x[0])\n",
    "        tmp = CompresorHibrido(5)\n",
    "        tmp.entrenar(' ')\n",
    "        \n",
    "        aux = rev_act.map(lambda x: crear_compresor(x)).reduce(lambda x,y: combinar_compresor(x, y))\n",
    "        \n",
    "        tmp.combinarTabla(aux.verTablas())\n",
    "        \n",
    "        #if (rev_act.count() > 0):\n",
    "        #    aux = rev_act.map(lambda x: crear_compresor(x)).reduce(lambda x,y: combinar_compresor(x, y))\n",
    "        #else:\n",
    "        #    aux= CompresorHibrido(5)\n",
    "        #    aux.entrenar(' ')\n",
    "        comp.append(tmp)           \n",
    "    return\n",
    "\n",
    "def asignar_puntuacion(texto):\n",
    "    return 1\n",
    "    global comp\n",
    "    minimo = -1\n",
    "    pred = 0\n",
    "    for i in range(0, 5):\n",
    "        act = comp[i].ratioCompresion(texto)\n",
    "        if ((minimo == -1) or (act < minimo)):\n",
    "            minimo = act\n",
    "            pred = i+1\n",
    "    return pred\n",
    "\n",
    "def predecir_comp(testSet):\n",
    "    setResultados = testSet.map(lambda x: (x[0], asignar_puntuacion(x[0])))\n",
    "    return setResultados\n",
    "\n",
    "comp = []\n",
    "\n",
    "# Primero guardo las reviews en formato (texto, puntaje) y pre-proceso\n",
    "reviews = data.map(lambda x: (x.Text, x.Prediction))\n",
    "\n",
    "# Hago k-fold crossed validation contra las reviews\n",
    "k_fold_crossed_validation(reviews, entrenar_comp, predecir_comp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "qwe = data.map(lambda x: x['Text'])\n",
    "qwe.take(1)\n",
    "comp = []\n",
    "comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reviews = data.map(lambda x: (x.Text, x.Prediction))\n",
    "rev_act = reviews.filter(lambda x: x[1] == 2).map(lambda x: x[0])\n",
    "rev_act = rev_act.map(lambda x: crear_compresor(x)).reduce(lambda x, y: combinar_compresor(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
