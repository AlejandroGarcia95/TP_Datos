{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lectura del archivo .csv de training\n",
    "import pyspark_csv as pycsv\n",
    "sc.addPyFile('pyspark_csv.py')\n",
    "plaintext_rdd = sc.textFile('train.csv')\n",
    "dataframe = pycsv.csvToDataFrame(sqlCtx, plaintext_rdd, parseDate=False)\n",
    "\n",
    "data = dataframe.rdd\n",
    "#data = data.sample(False, 0.005)\n",
    "#data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Llevo a que la clave sea el texto de la review\n",
    "data = data.map(lambda x: (x[0][9],(x[0][0],x[0][1],x[0][2],x[0][3],x[0][4],x[0][5],x[0][6],x[0][7],x[0][8])))\n",
    "data = data.reduceByKey(lambda x,y: (x[0],x[1])) #Guardo el primer registro de esa review\n",
    "#Devuelvo a formato original\n",
    "data = data.map(lambda x: (x[1][0],x[1][1],x[1][2],x[1][3],x[1][4],x[1][5],x[1][6],x[1][7],x[1][8],x[0]))\n",
    "\n",
    "#Por algún motivo marca error la línea siguiente\n",
    "#data.take(1)\n",
    "#A pesar de que esta linea tira pyspark.rdd.PipelinedRDD\n",
    "type(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Funcion para en principio formar el csv. En chequeo\n",
    "def toCSVLine(data):\n",
    "  return ','.join(str(d) for d in data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # Para eliminar tags html\n",
    "import re # Expresiones regulares para eliminar puntuacion\n",
    "from nltk.corpus import stopwords # Stopwords para eliminar palabras comunes\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "def aplicarStemming(x):\n",
    "    words = x.split()\n",
    "    st = LancasterStemmer()\n",
    "    new_words = []\n",
    "    for w in words:\n",
    "        new_words.append(st.stem(w))\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "def borrarPalabrasComunes(x):\n",
    "    words = x.split()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    stop_words.remove(\"not\")\n",
    "    stop_words.remove(\"hadn\")\n",
    "    stop_words.remove(\"hasn\")\n",
    "    stop_words.remove(\"didn\")\n",
    "    new_words = []\n",
    "    for w in words:\n",
    "        if(not w in stop_words):\n",
    "            new_words.append(w)\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "def eliminarMenosUsadas(x):\n",
    "    global menosUsadas\n",
    "    print menosUsadas\n",
    "    words = x.split()    \n",
    "    new_words = []\n",
    "    for w in words:\n",
    "        if(not w in menosUsadas):\n",
    "            new_words.append(w)\n",
    "    return \" \".join(new_words)\n",
    "    \n",
    "def borrarSimbolos(x):\n",
    "    aBorrar = \",@#$-.():[]!?'\"\n",
    "    for c in aBorrar:\n",
    "        x = x.replace(c, \"\")\n",
    "    return x\n",
    "\n",
    "def explicitarNegacion(x):\n",
    "    negadores = [\"not\", \"no\", \"dont\", \"doesnt\", \"havent\", \"hasnt\", \"isnt\", \"arent\"\n",
    "                \"wont\", \"aint\", \"didnt\", \"hadnt\"]\n",
    "    words = x.split()\n",
    "    new_words = []\n",
    "    i = 0\n",
    "    while(i < len(words)):\n",
    "        if(words[i] in negadores and (i<(len(words)-1))):\n",
    "            new_words.append(\"NOT_\"+words[i+1].upper())\n",
    "            i += 1\n",
    "        else:\n",
    "            new_words.append(words[i])\n",
    "        i += 1\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "# El formato de n-gramas es el siguiente: los n-gramas se devuelven en un\n",
    "# string, separados cada uno por un espacio. Para considerar los espacios\n",
    "# \"verdaderos\" del texto original, se los reemplaza primero por @.\n",
    "# Por ejemplo, el string \"Un buen dia\" se traduce con esta función, y tam.\n",
    "# de n-grama=3 al string \"Un@ n@b @bu bue uen en@ n@d @di dia\".\n",
    "def conseguirNgramas(x):\n",
    "    ngramSize = 3\n",
    "    old_string = x.replace(\" \", \"@\")\n",
    "    if(len(old_string) < ngramSize):\n",
    "        return old_string\n",
    "    new_string = old_string[0:ngramSize]\n",
    "    for i in range(1, len(old_string)-ngramSize+1):\n",
    "        new_string += \" \"\n",
    "        new_string += old_string[i:(i+ngramSize)]\n",
    "    return new_string\n",
    "\n",
    "def considerarEmoticonesPuntuacion(x):\n",
    "    # Lista de caritas felices\n",
    "    caras_felices = [\":)\", \"(:\", \"[:\", \":]\", \"c:\", \"=)\", \"=]\", \"(=\", \"[=\", \"c=\",\n",
    "                    \"=D\", \":D\", \";)\", \"(;\", \";D\"]\n",
    "    for emoji in caras_felices:\n",
    "        x = x.replace(emoji , \"SMILING_FACE\")\n",
    "    # Lista de caritas tristes\n",
    "    caras_tristes = [\":(\", \":[\", \"):\", \"]:\", \":c\", \"=(\", \"=[\", \"]=\", \"=c\", \"D=\", \n",
    "                    \"D:\", \";(\", \");\", \"D;\", ]\n",
    "    for emoji in caras_tristes:\n",
    "        x = x.replace(emoji, \"SAD_FACE\")\n",
    "    # Lista de caritas sorprendidas\n",
    "    caras_sorpr = [\":0\", \":o\", \"0:\", \"o:\", \"=o\", \"0=\"]\n",
    "    for emoji in caras_sorpr:\n",
    "        x = x.replace(emoji, \"SURPRISED_FACE\") \n",
    "    # Puntuación (signos ! y ?)\n",
    "    x = x.replace(\"!!!\", \" ADMIRx3\")\n",
    "    x = x.replace(\"!!\", \" ADMIRx2\")\n",
    "    x = x.replace(\"???\", \" QUESx3\")\n",
    "    x = x.replace(\"??\", \" QUESx2\")\n",
    "    x = x.replace(\"?!\", \" ADM_QUES\")\n",
    "    x = x.replace(\"!?\", \" ADM_QUES\")\n",
    "    x = x.replace(\"!\", \" ADMIRx1\")\n",
    "    x = x.replace(\"?\", \" QUESx1\")\n",
    "    return x\n",
    "\n",
    "# Función encargada de realizar un pre-procesamiento de los textos de las reviews\n",
    "# según lo considerado por nuestro diseño del TP. Para ello, se recibe el set de\n",
    "# entrenamiento como un RDD de reviews, que son tuplas (texto, puntaje).\n",
    "# Las distintas acciones que la función realiza sobre el texto de las reviews\n",
    "# dependen de los flags de procesamiento recibidos en flagsP (como lista).\n",
    "# A continuación la lista de acciones controlada por cada flag de flagsP:\n",
    "# flagsP[0] controla la eliminación de palabras comunes (\"a\", \"the\", \"of\", etc.)\n",
    "# fragsP[1] elimina las palabras de frecuencia menor a *elMinimo*\n",
    "# flagsP[2] activa el uso de stemming sobre las palabras de la review\n",
    "# flagsP[3] activa el reconocimiento de emoticones y puntuaciones ?,!\n",
    "# flagsP[4] activa la explicitación de la negación\n",
    "# flagsP[5] convierte el texto a n-gramas\n",
    "# Notar que las acciones se hacen en el orden explicitado por los flags (primero se eliminan\n",
    "# las palabras comunes, después las de frecuencia menor, después stemming, etc.)\n",
    "# Acciones que el pre-procesador de reviews hace siempre:\n",
    "# - Eliminar tags html\n",
    "# - Convertir todo a minúsculas\n",
    "# - Eliminar los siguientes símbolos: \",\" \"@\" \"#\" \"$\" \"-\" \".\" \"(\" \")\" \":\" \"]\" \"[\"\n",
    "# (En el caso de considerar emoticones o puntuación no lo hace hasta después de\n",
    "# detectar todos los emoticones o símbolos deseados correspondientes)\n",
    "def preprocesar_reviews(elSet, flagsP):\n",
    "    nuevoSet = elSet.map(lambda x: (BeautifulSoup(x[0], \"lxml\").getText(), x[1]) )\n",
    "    nuevoSet = nuevoSet = nuevoSet.map(lambda x: (x[0].lower(), x[1]))\n",
    "    \n",
    "    if(flagsP[0]):\n",
    "        nuevoSet = nuevoSet.map(lambda x: (borrarPalabrasComunes(x[0]), x[1]))\n",
    "        \n",
    "    if(flagsP[1]):\n",
    "        global menosUsadas\n",
    "        elMinimo = 10\n",
    "        setFrec = nuevoSet.flatMap(lambda x: x[0].split()).map(lambda x: (x, 1))\n",
    "        setFrec = setFrec.reduceByKey(lambda x,y: x+y)\n",
    "        menosUsadas = setFrec.filter(lambda x: x[1] < elMinimo).map(lambda x: x[0]).collect()\n",
    "        nuevoSet = nuevoSet.map(lambda x: (eliminarMenosUsadas(x[0]), x[1]))\n",
    "    \n",
    "    if(flagsP[2]):\n",
    "        nuevoSet = nuevoSet.map(lambda x: (aplicarStemming(x[0]), x[1]))\n",
    "    \n",
    "    if(flagsP[3]):\n",
    "        nuevoSet = nuevoSet.map(lambda x: (considerarEmoticonesPuntuacion(x[0]), x[1]))\n",
    "    nuevoSet = nuevoSet = nuevoSet.map(lambda x: (borrarSimbolos(x[0]), x[1]))\n",
    "    \n",
    "    if(flagsP[4]):\n",
    "        nuevoSet = nuevoSet.map(lambda x: (explicitarNegacion(x[0]), x[1]))\n",
    "    \n",
    "    if(flagsP[5]):\n",
    "        nuevoSet = nuevoSet.map(lambda x: (conseguirNgramas(x[0]), x[1]))\n",
    "    \n",
    "    return nuevoSet\n",
    "\n",
    "menosUsadas = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Todo este bloque define la realización del k-fold crossed validation.\n",
    "# El método funciona así: recibe un set de entrenamiento y hace sobre el\n",
    "# mismo la técnica de k-fold crossed validation. El formato del set debe\n",
    "# ser un RDD de TUPLAS de la forma: (features, categoria) donde la clave\n",
    "# features puede ser cualquier basura, y categoria es el valor numérico\n",
    "# que se desea predecir (aka el puntaje de cada review). En cada pasada\n",
    "# del k-fold crossed validation, se invocan a las funciones de entrenar\n",
    "# func_entrenar y a las de predicción func_predecir. Éstas dos funciones\n",
    "# deben trabajar de manera global con el/los compresor/es o el SVM. Sus\n",
    "# firmas deben ser las siguientes:\n",
    "# func_entrenar recibe un set de entrenamiento (en el mismo formato que\n",
    "# el set original, como tuplas feature,cat.) y prepara al compresor o SVM\n",
    "# para las predicciones usando ese set.\n",
    "# func_predecir recibe un set a predecir (en el mismo formato de tuplas\n",
    "# feature, cat) y debe devolver OTRO set (también en el mismo formato!)\n",
    "# que correspondan a las predicciones hechas por el SVM o compresores.\n",
    "# Observación importante: como el k-fold crossed validation en sí no\n",
    "# tiene ni idea qué usamos para predecir, todo lo demás ajeno a eso,\n",
    "# incluyendo la selección de hiperparametros, debe hacerse \"por fuera\",\n",
    "# ya sea con un pre-procesamiento de las reviews o en la función de entrenar.\n",
    "\n",
    "def fooCount(x):\n",
    "    global contadora\n",
    "    contadora += 1\n",
    "    return (x, contadora)\n",
    "\n",
    "def calculo_ECM(predSet, valSet,cant):\n",
    "    #cant = predSet.count()\n",
    "    #cant = 1\n",
    "    setAux = predSet.union(valSet)\n",
    "   # setAux = setAux.map(lambda x: (np.array_str(x[0]), x[1]) )\n",
    "    setAux = setAux.reduceByKey(lambda x,y: float(x)-float(y)).map(lambda x: x[1]*x[1])\n",
    "    ecm = setAux.reduce(lambda x,y: x+y)\n",
    "    return (ecm/float(cant))\n",
    "\n",
    "def k_fold_crossed_validation(elSet, func_entrenar, func_predecir):\n",
    "    cantParticiones = 8\n",
    "    ecm_acum = 0.0\n",
    "    largoSet = elSet.count()\n",
    "    largoParticion = largoSet / cantParticiones\n",
    "    setauxi = elSet.map(fooCount)\n",
    "    for j in range (1, cantParticiones+1):\n",
    "        # Obtengo el testSet como la particion j-ésima y el trainSet como\n",
    "        # todo el resto del set recibido menos el testSet\n",
    "        #print \"El set:\", elSet.take(2)\n",
    "        testSet = setauxi.filter(lambda x: (x[1] % cantParticiones) == (j-1)).map(lambda x: x[0])\n",
    "        trainSet = setauxi.filter(lambda x: (x[1] % cantParticiones) != (j-1)).map(lambda x: x[0])\n",
    "        #print \"Antes de entrenar:\", trainSet.take(2)\n",
    "        # Entreno contra trainSet\n",
    "        func_entrenar(trainSet)\n",
    "        # Testeo contra testSet\n",
    "        setResultados = func_predecir(testSet)\n",
    "        cantid = largoSet/cantParticiones\n",
    "        ecm_acum += calculo_ECM(setResultados, testSet,cantid)\n",
    "        print \"ECM acumulado iteracion\", j, \"es:\", ecm_acum\n",
    "    # Obtengo el ECM promedio de la validación\n",
    "    print \"ECM promedio:\", (ecm_acum/float(cantParticiones))\n",
    "    \n",
    "    \n",
    "contadora = 0 # Se usa, no tocar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECM acumulado iteracion 1 es: 2.09594095941\n",
      "ECM acumulado iteracion 2 es: 4.16236162362\n",
      "ECM acumulado iteracion 3 es: 6.68634686347\n",
      "ECM acumulado iteracion 4 es: 9.46125461255\n",
      "ECM acumulado iteracion 5 es: 11.9114391144\n",
      "ECM acumulado iteracion 6 es: 13.5867158672\n",
      "ECM acumulado iteracion 7 es: 15.9778597786\n",
      "ECM acumulado iteracion 8 es: 18.4095940959\n",
      "ECM promedio: 2.30119926199\n"
     ]
    }
   ],
   "source": [
    "from ppmc import *\n",
    "import numpy as np\n",
    "import gc\n",
    "def crear_compresor(x):\n",
    "    aux = CompresorHibrido(5)\n",
    "    aux.entrenar(x)\n",
    "    return aux\n",
    "\n",
    "def combinar_compresor(x, y):\n",
    "    x.combinarTabla(y.verTablas())\n",
    "    return x\n",
    "\n",
    "def entrenar_comp(trainSet):\n",
    "    global comp\n",
    "    comp = []\n",
    "    gc.collect()\n",
    "    for i in range(1, 6): # 5 puntajes posibles\n",
    "        rev_act = trainSet.filter(lambda x: x[1] == i).map(lambda x: x[0])\n",
    "        tmp = CompresorHibrido(3)\n",
    "        tmp.entrenar(' ')\n",
    "        \n",
    "        #aux2 = rev_act.map(lambda x: crear_compresor(x)).reduce(lambda x,y: combinar_compresor(x, y))\n",
    "        concat = rev_act.reduce(lambda x,y: (x + \"\\n\\n\\n\\n\\n\"+y))\n",
    "        tmp.entrenar(concat)\n",
    "        \n",
    "        #tmp.combinarTabla(aux2.verTablas())\n",
    "        \n",
    "        comp.append(tmp)           \n",
    "    return\n",
    "\n",
    "def asignar_puntuacion(texto):\n",
    "    global comp\n",
    "    minimo = -1\n",
    "    pred = 0\n",
    "    for i in range(0, 5):\n",
    "        act = comp[i].ratioCompresion(texto)\n",
    "        if ((minimo == -1) or (act < minimo)):\n",
    "            minimo = act\n",
    "            pred = i+1\n",
    "    return pred\n",
    "\n",
    "def predecir_comp(testSet):\n",
    "    setResultados = testSet.map(lambda x: (x[0], asignar_puntuacion(x[0])))\n",
    "    return setResultados\n",
    "\n",
    "comp = []\n",
    "\n",
    "# Primero guardo las reviews en formato (texto, puntaje) y pre-proceso\n",
    "reviews = data.map(lambda x: (x.Text, x.Prediction)).map(lambda x: (x[0].encode(\"ascii\", \"ignore\"),x[1]))\n",
    "\n",
    "# Hago k-fold crossed validation contra las reviews\n",
    "k_fold_crossed_validation(reviews, entrenar_comp, predecir_comp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qwe = data.map(lambda x: x['Text'])\n",
    "qwe.take(1)\n",
    "comp = []\n",
    "comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reviews = data.map(lambda x: (x.Text, x.Prediction))\n",
    "rev_act = reviews.filter(lambda x: x[1] == 2).map(lambda x: x[0])\n",
    "rev_act = rev_act.map(lambda x: crear_compresor(x)).reduce(lambda x, y: combinar_compresor(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2172"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Id=385774, ProductId=u'B00434HR64', UserId=u'A3OFVWDDEJGMNN', ProfileName=u'Meh123', HelpfulnessNumerator=1, HelpfulnessDenominator=1, Prediction=1, Time=1324339200, Summary=u'Dessert and Entree all in one!', Text=u'My husband picked this up thinking it would be better since it was whole grain. I am not a particular eater, and I\\'ve eaten Chicken Helper before, but I really did not like this flavor. I don\\'t know if Betty Crocker was aiming for Italian, or Oriental, or what. It came with two seasoning packets. The first one wasn\\'t so bad, it\\'s the one you add later, the \"glaze\" that really ruined the dish. The \"glaze\" tastes exactly like lemon pudding mix. So the end result was a salty chicken casserole with a hint of sweet lemon meringue pie. No thanks!'),\n",
       " Row(Id=166015, ProductId=u'B007FRAVNC', UserId=u'A1USOFWS1ZE1F', ProfileName=u'Dorothy Daidone', HelpfulnessNumerator=1, HelpfulnessDenominator=1, Prediction=5, Time=1255910400, Summary=u'Great Tea', Text=u'This tea is very good, and I love the health benefits that you get from it. Very happy with Teavana.')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark",
   "language": "python",
   "name": "pyspark"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
