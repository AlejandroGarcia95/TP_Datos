{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lectura del archivo .csv de training\n",
    "import pyspark_csv as pycsv\n",
    "sc.addPyFile('pyspark_csv.py')\n",
    "plaintext_rdd = sc.textFile('muestra100.csv')\n",
    "dataframe = pycsv.csvToDataFrame(sqlCtx, plaintext_rdd, parseDate=False)\n",
    "\n",
    "\n",
    "data = dataframe.rdd\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Esta celda ya expiró (ni mirar)\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from MulticatSVM import *\n",
    "import numpy as np\n",
    "\n",
    "def parsearReview(x):\n",
    "    parser = SymbolParser('palabra')\n",
    "    texto = x[0]\n",
    "    tabla = x[1]\n",
    "    parser.parsearTexto(texto, tabla)\n",
    "    return tabla\n",
    "\n",
    "def mergeSymbolTables(tablaA, tablaB):\n",
    "    # Copio A en C\n",
    "    tablaC = SymbolTable()\n",
    "    for simb in tablaA.verSimbolos():\n",
    "        freq = tablaA.verFrecuencia(simb)\n",
    "        while (freq > 1):\n",
    "            tablaC.aumentarFrecuencia(simb)\n",
    "            freq -= 1  \n",
    "    # Veo ahora los simbolos en B\n",
    "    for simb in tablaB.verSimbolos():\n",
    "        freq = tablaB.verFrecuencia(simb)\n",
    "        while (freq > 1):\n",
    "            tablaC.aumentarFrecuencia(simb)\n",
    "            freq -= 1    \n",
    "    return tablaC\n",
    "\n",
    "def vectorizar_reviews(elSet):\n",
    "    tablas = elSet.map(lambda x: (x[0], SymbolTable())).map(parsearReview)\n",
    "    tablas = tablas.map(lambda x: dict(x.verItems())).collect()\n",
    "\n",
    "\n",
    "reviews = data.map(lambda x: x.Text)\n",
    "\n",
    "desayunos = [\"cafe medialunas torta\", \"cafe cafe te\", \"medialunas torta medialunas\", \n",
    "     \"te torta\", \"cafe medialunas medialunas\", \"te torta torta\", \"cafe torta\"]\n",
    "almuerzos = [\"fideos carne\", \"milanesa carne\", \"fideos milanesa\", \"arroz milanesa\",\n",
    "            \"fideos milanesa carne\", \"carne arroz arroz\", \"fideos fideos\", \"milanesa\"]\n",
    "reviews = sc.parallelize(desayunos, 4) # Matar post ejemplo\n",
    "tablas = reviews.map(lambda x: (x, SymbolTable())).map(parsearReview)\n",
    "tablas = tablas.map(lambda x: dict(x.verItems())).collect()\n",
    "#print tablas\n",
    "vectorizador = DictVectorizer(sparse=False)\n",
    "desayuno_vectorizado = vectorizador.fit_transform(tablas)\n",
    "pepe = []\n",
    "for v in range (0, len(desayuno_vectorizado)):\n",
    "    pepe.append(np.append(desayuno_vectorizado[v], np.array([0.0, 0.0, 0.0, 0.0])))\n",
    "desayuno_vectorizado = pepe\n",
    "\n",
    "print desayuno_vectorizado\n",
    "\n",
    "# Matar después:\n",
    "# Desayuno: categoría 1; Almuerzo: categoría 2\n",
    "reviews = sc.parallelize(almuerzos, 4) # Matar post ejemplo\n",
    "tablas = reviews.map(lambda x: (x, SymbolTable())).map(parsearReview)\n",
    "tablas = tablas.map(lambda x: dict(x.verItems())).collect() \n",
    "vectorizador = DictVectorizer(sparse=False)\n",
    "almuerzo_vectorizado = vectorizador.fit_transform(tablas)\n",
    "pepe = []\n",
    "for v in range (0, len(almuerzo_vectorizado)):\n",
    "    pepe.append(np.append(np.array([0.0, 0.0, 0.0, 0.0]), almuerzo_vectorizado[v]))\n",
    "almuerzo_vectorizado = pepe\n",
    "print almuerzo_vectorizado\n",
    "\n",
    "comidas = []\n",
    "comidas.append(desayuno_vectorizado)\n",
    "comidas.append(almuerzo_vectorizado)\n",
    "\n",
    "our_svm = MulticatSVM(dim = 8, cte_soft_margin = 9000.0, cantCategorias = 2)\n",
    "our_svm.entrenar(comidas)\n",
    "\n",
    "print \"Categoria desayuno:\", our_svm.predecir(np.array([1., 1., 0., 2., 0., 0., 0., 0.]))\n",
    "print \"Categoria almuerzo:\", our_svm.predecir(np.array([0., 0., 0., 0., 1., 0., 2., 0.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from compresores import *\n",
    "\n",
    "\n",
    "# cmpr es una lista de compresores estáticos, donde el compresor i\n",
    "# es encargado de comprimir reviews de puntaje i+1\n",
    "cmpr = []\n",
    "for i in range(0,5):\n",
    "    cmpr.append(CompresorAritmetico('letra'))  \n",
    "        \n",
    "# Del set de datos, obtengo para compresor i las reviews de\n",
    "# puntaje i+1 y las parseo para actualizar las tablas de símbolos\n",
    "for i in range(0,5):\n",
    "    reviews = data.filter(lambda x: x.Prediction == (i+1)).map(lambda x: x.Text)\n",
    "    reviews = reviews.map(lambda x: (x, SymbolTable())).map(parsearReview)\n",
    "    # Ahora en reviews hay tablas de símbolos que representan\n",
    "    # las reviews originales. ¡Las mergeamos!\n",
    "    tablaFinal = reviews.reduce(mergeSymbolTables)\n",
    "    cmpr[i].asignarSymbolTable(tablaFinal)\n",
    "\n",
    "# Pruebo mostrando los simbolos de cada compresor\n",
    "for i in range(0,5):\n",
    "    print 'Símbolos del conversor', i, ':', cmpr[i].getSymbolTable().verSimbolos()\n",
    "    print 'Frecuencia de la a:', cmpr[i].getSymbolTable().verFrecuencia('a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DE ACA PARA ABAJO ZONA DE TESTS\n",
    "# NO DAR MUCHA BOLA\n",
    "\n",
    "\n",
    "\n",
    "print lasReviews.takeOrdered(1)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = lasReviews.map(lambda x: (x[0],x[1].count(\"!!! \"))).reduceByKey(lambda x, y: x+y)\n",
    "print q.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from enchant.checker import SpellChecker\n",
    "chkr = SpellChecker(\"en_US\")\n",
    "\n",
    "reviewsAlAzar = textos.takeSample(False, 5)\n",
    "\n",
    "\n",
    "for r in range(0,5):\n",
    "    print reviewsAlAzar[r]\n",
    "    chkr.set_text(reviewsAlAzar[r])\n",
    "    for err in chkr:\n",
    "        print \"ERROR! Wrong word:\", err.word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Todo este bloque define la realización del k-fold crossed validation.\n",
    "# El método funciona así: recibe un set de entrenamiento y hace sobre el\n",
    "# mismo la técnica de k-fold crossed validation. El formato del set debe\n",
    "# ser un RDD de TUPLAS de la forma: (features, categoria) donde la clave\n",
    "# features puede ser cualquier basura, y categoria es el valor numérico\n",
    "# que se desea predecir (aka el puntaje de cada review). En cada pasada\n",
    "# del k-fold crossed validation, se invocan a las funciones de entrenar\n",
    "# func_entrenar y a las de predicción func_predecir. Éstas dos funciones\n",
    "# deben trabajar de manera global con el/los compresor/es o el SVM. Sus\n",
    "# firmas deben ser las siguientes:\n",
    "# func_entrenar recibe un set de entrenamiento (en el mismo formato que\n",
    "# el set original, como tuplas feature,cat.) y prepara al compresor o SVM\n",
    "# para las predicciones usando ese set.\n",
    "# func_predecir recibe un set a predecir (en el mismo formato de tuplas\n",
    "# feature, cat) y debe devolver OTRO set (también en el mismo formato!)\n",
    "# que correspondan a las predicciones hechas por el SVM o compresores.\n",
    "# Observación importante: como el k-fold crossed validation en sí no\n",
    "# tiene ni idea qué usamos para predecir, todo lo demás ajeno a eso,\n",
    "# incluyendo la selección de hiperparametros, debe hacerse \"por fuera\",\n",
    "# ya sea con un pre-procesamiento de las reviews o en la función de entrenar.\n",
    "\n",
    "def calculo_ECM(predSet, valSet):\n",
    "    cant = predSet.count()\n",
    "    setAux = predSet.union(valSet)\n",
    "    setAux = setAux.reduceByKey(lambda x,y: float(x)-float(y)).map(lambda x: x[1]*x[1])\n",
    "    ecm = setAux.reduce(lambda x,y: x+y)\n",
    "    return (ecm/float(cant))\n",
    "\n",
    "def k_fold_crossed_validation(elSet, func_entrenar, func_predecir):\n",
    "    cantParticiones = 10\n",
    "    ecm_acum = 0.0\n",
    "    largoSet = elSet.count()\n",
    "    largoParticion = largoSet / cantParticiones\n",
    "    for j in range (1, cantParticiones+1):\n",
    "        # Obtengo el testSet como la particion j-ésima y el trainSet como\n",
    "        # todo el resto del set recibido menos el testSet\n",
    "        testSet = sc.parallelize(elSet.take(largoParticion * j), 4)\n",
    "        testSet = testSet.subtract(sc.parallelize(elSet.take(largoParticion * (j-1)), 4))\n",
    "        trainSet = elSet.subtract(testSet)\n",
    "        # Entreno contra trainSet\n",
    "        func_entrenar(trainSet)\n",
    "        # Testeo contra testSet\n",
    "        setResultados = func_predecir(testSet)\n",
    "        ecm_acum += calculo_ECM(setResultados, testSet)\n",
    "        print \"ECM acumulado iteracion\", j, \"es:\", ecm_acum\n",
    "    # Obtengo el ECM promedio de la validación\n",
    "    print \"ECM promedio:\", (ecm_acum/float(cantParticiones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def parsearReview(x):\n",
    "    parser = SymbolParser('palabra')\n",
    "    texto = x[0]\n",
    "    tabla = x[1]\n",
    "    parser.parsearTexto(texto, tabla)\n",
    "    return tabla\n",
    "\n",
    "# Método auxiliar usado en el map-reduce: Fusiona dos diccionarios\n",
    "def unirDiccionarios(dicA, dicB):\n",
    "    # Copio A en C\n",
    "    dicC = {}\n",
    "    for clave in dicA.keys():\n",
    "        valor = dicA[clave]\n",
    "        dicC[clave] = valor\n",
    "    # Veo ahora los simbolos en B\n",
    "    for clave in dicB.keys():\n",
    "        valor = dicB[clave]\n",
    "        if dicC.has_key(clave):\n",
    "            dicC[clave] += valor\n",
    "        else:\n",
    "            dicC[clave] = valor\n",
    "    return dicC\n",
    "\n",
    "# Método auxiliar de vectorización: Genera un numpy.vector que representa\n",
    "# la cant. de palabras de un determinado review.\n",
    "def vectorizar(x):\n",
    "    global tabla_vec\n",
    "    words = tabla_vec.keys()\n",
    "    p = []\n",
    "    for w in words:\n",
    "        if(x.has_key(w)):\n",
    "            p.append(float(x[w]))\n",
    "        else:\n",
    "            p.append(0.0)\n",
    "    return np.array(p)\n",
    "\n",
    "# vectorizar_reviews es la función encargada de convertir en numpy.vector\n",
    "# todos los textos de todas las reviews. Para ello, recibe como parámetro\n",
    "# el set de entrenamiento, que debe tener el sge formato: debe ser un RDD\n",
    "# formado por tuplas (texto, puntaje) de cada review. El método devuelve\n",
    "# otro RDD con el formato (vector, puntaje) donde vector es un numpy.vector\n",
    "# que representa el texto de cada review recibida. Cada vector se consigue\n",
    "# de la sgte forma: Primero se listan todas las palabras de todas las\n",
    "# reviews del set, y se les asigna a cada una de ellas un índice del vector; \n",
    "# luego, para cada review del set, se cuentan cuántas palabras hay y cuáles,\n",
    "# y se ponen esos valores de contadores en las posiciones correspondientes\n",
    "# del vector. Por ejemplo, si todas las palabras son [\"Casa\", \"Pez\", \"Arbol\"]\n",
    "# una review de la forma \"Casa Pez Casa\" se traducirá como el vector (2, 1, 0)\n",
    "# NOTA: Si se quiere que en vez de contar las palabras se cargue sólamente un\n",
    "# 1 o un 0 según si la palabra está presente o no, se debe cambiar en la\n",
    "# función vectorizar de arriba el p.append(float(x[w])) por un p.append(1.0)\n",
    "# ADVERTENCIA: El vectorizador no hace ningún procesamiento del texto, y\n",
    "# separa sólo las palabras por espacios. Cualquier pre-procesamiento (por\n",
    "# ejemplo, eliminar las \",\") debe hacerse antes de llamarlo.\n",
    "\n",
    "def vectorizar_reviews(elSet):\n",
    "    # Primero consigo en tabla_vec un diccionario con todas las palabras\n",
    "    # de todas las reviews del set\n",
    "    global tabla_vec\n",
    "    tabla_vec = elSet.map(lambda x: (x[0], SymbolTable())).map(parsearReview)\n",
    "    tabla_vec = tabla_vec.map(lambda x: dict(x.verItems()))\n",
    "    tabla_vec = tabla_vec.reduce(unirDiccionarios)\n",
    "    # Vectorizo ahora todas las reviews del set usando esa tabla\n",
    "    reviews_vec = reviews.map(lambda x: (x[0], SymbolTable(), x[1]))\n",
    "    reviews_vec = reviews_vec.map(lambda x: (parsearReview(x), x[2]))\n",
    "    reviews_vec = reviews_vec.map(lambda x: (dict(x[0].verItems()), x[1]))\n",
    "    reviews_vec = reviews_vec.map(lambda x: (vectorizar(x[0]), x[1]))\n",
    "    return reviews_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # Para eliminar tags html\n",
    "import re # Expresiones regulares para eliminar puntuacion\n",
    "from nltk.corpus import stopwords # Stopwords para eliminar palabras comunes\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "def aplicarStemming(x):\n",
    "    words = x.split()\n",
    "    st = LancasterStemmer()\n",
    "    new_words = []\n",
    "    for w in words:\n",
    "        new_words.append(st.stem(w))\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "def borrarPalabrasComunes(x):\n",
    "    words = x.split()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    stop_words.remove(\"not\")\n",
    "    new_words = []\n",
    "    for w in words:\n",
    "        if(not w in stop_words):\n",
    "            new_words.append(st.stem(w))\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "def borrarSimbolos(x):\n",
    "    aBorrar = \",@#$-.():[]!?\"\n",
    "    for c in aBorrar:\n",
    "        x = x.replace(c, \" \")\n",
    "    return x\n",
    "\n",
    "def considerarEmoticonesPuntuacion(x):\n",
    "    # Lista de caritas felices\n",
    "    caras_felices = [\":)\", \"(:\", \"[:\", \":]\", \"c:\", \"=)\", \"=]\", \"(=\", \"[=\", \"c=\",\n",
    "                    \"=D\", \":D\", \";)\", \"(;\", \";D\"]\n",
    "    for emoji in caras_felices:\n",
    "        x = x.replace(emoji , \"SMILING_FACE\")\n",
    "    # Lista de caritas tristes\n",
    "    caras_tristes = [\":(\", \":[\", \"):\", \"]:\", \":c\", \"=(\", \"=[\", \"]=\", \"=c\", \"D=\", \n",
    "                    \"D:\", \";(\", \");\", \"D;\", ]\n",
    "    for emoji in caras_tristes:\n",
    "        x = x.replace(emoji, \"SAD_FACE\")\n",
    "    # Lista de caritas sorprendidas\n",
    "    caras_sorpr = [\":0\", \":o\", \"0:\", \"o:\", \"=o\", \"0=\"]\n",
    "    for emoji in caras_sorpr:\n",
    "        x = x.replace(emoji, \"SURPRISED_FACE\") \n",
    "    # Puntuación (signos ! y ?)\n",
    "    x = x.replace(\"!!!\", \" ADMIRx3\")\n",
    "    x = x.replace(\"!!\", \" ADMIRx2\")\n",
    "    x = x.replace(\"???\", \" QUESx3\")\n",
    "    x = x.replace(\"??\", \" QUESx2\")\n",
    "    x = x.replace(\"?!\", \" ADM_QUES\")\n",
    "    x = x.replace(\"!?\", \" ADM_QUES\")\n",
    "    x = x.replace(\"!\", \" ADMIRx1\")\n",
    "    x = x.replace(\"?\", \" QUESx1\")\n",
    "    return x\n",
    "\n",
    "# Función encargada de realizar un pre-procesamiento de los textos de las reviews\n",
    "# según lo considerado por nuestro diseño del TP. Para ello, se recibe el set de\n",
    "# entrenamiento como un RDD de reviews, que son tuplas (texto, puntaje).\n",
    "# Las distintas acciones que la función realiza sobre el texto de las reviews\n",
    "# dependen de los flags de procesamiento recibidos en flagsP (como lista).\n",
    "# A continuación la lista de acciones controlada por cada flag de flagsP:\n",
    "# flagsP[0] controla la eliminación de palabras comunes (\"a\", \"the\", \"of\", etc.)\n",
    "# fragsP[1] elimina las palabras de frecuencia menor a *un número*\n",
    "# flagsP[2] activa el uso de stemming sobre las palabras de la review\n",
    "# flagsP[3] activa el reconocimiento de emoticones y puntuaciones ?,!\n",
    "# continuar...\n",
    "# Acciones que el pre-procesador de reviews hace siempre:\n",
    "# - Eliminar tags html\n",
    "# - Convertir todo a minúsculas\n",
    "# - Eliminar los siguientes símbolos: \",\" \"@\" \"#\" \"$\" \"-\" \".\" \"(\" \")\" \":\" \"]\" \"[\"\n",
    "# (En el caso de considerar emoticones o puntuación no lo hace hasta después de\n",
    "# detectar todos los emoticones o símbolos deseados correspondientes)\n",
    "def preprocesar_reviews(elSet, flagsP):\n",
    "    nuevoSet = elSet.map(lambda x: (BeautifulSoup(x[0], \"lxml\").getText(), x[1]) )\n",
    "    nuevoSet = nuevoSet = nuevoSet.map(lambda x: (x[0].lower(), x[1]))\n",
    "    \n",
    "    if(flagsP[0]):\n",
    "        nuevoSet = nuevoSet.map(lambda x: (borrarPalabrasComunes(x[0]), x[1]))\n",
    "    \n",
    "    if(flagsP[2]):\n",
    "        nuevoSet = nuevoSet.map(lambda x: (aplicarStemming(x[0]), x[1]))\n",
    "    \n",
    "    if(flagsP[2]):\n",
    "        nuevoSet = nuevoSet.map(lambda x: (considerarEmoticonesPuntuacion(x[0]), x[1]))\n",
    "    nuevoSet = nuevoSet = nuevoSet.map(lambda x: (borrarSimbolos(x[0]), x[1]))\n",
    "    \n",
    "    \n",
    "    return nuevoSet\n",
    "\n",
    "reviews = data.map(lambda x: (x.Text, x.Prediction))\n",
    "print reviews.count()\n",
    "reviews = preprocesar_reviews(reviews, [1, 0, 1, 1])\n",
    "print reviews.count()\n",
    "print reviews.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "st = SnowballStemmer(\"english\")\n",
    "cade = 'maximum provision for our maxim northern allies'\n",
    "print cade\n",
    "for w in cade.split():\n",
    "    print st.stem(w)\n",
    "print cade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteracion 1 : 0.000102043151855\n",
      "Iteracion 2 : 6.103515625e-05\n",
      "Iteracion 3 : 4.19616699219e-05\n",
      "Iteracion 4 : 4.60147857666e-05\n",
      "Iteracion 5 : 3.81469726562e-05\n",
      "Iteracion 6 : 6.38961791992e-05\n",
      "Iteracion 7 : 4.10079956055e-05\n",
      "Iteracion 8 : 3.69548797607e-05\n",
      "Iteracion 9 : 0.000159025192261\n",
      "Iteracion 10 : 3.81469726562e-05\n",
      "Iteracion 11 : 3.79085540771e-05\n",
      "Iteracion 12 : 9.79900360107e-05\n",
      "Iteracion 13 : 3.88622283936e-05\n",
      "Iteracion 14 : 3.71932983398e-05\n",
      "Iteracion 15 : 4.41074371338e-05\n",
      "Iteracion 16 : 3.91006469727e-05\n",
      "Iteracion 17 : 3.60012054443e-05\n",
      "Iteracion 18 : 7.31945037842e-05\n",
      "Iteracion 19 : 0.000108003616333\n",
      "Iteracion 20 : 5.07831573486e-05\n",
      "Promedio: 5.95688819885e-05\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import time\n",
    "import math\n",
    "\n",
    "def kernel_gaussiano(x, y):\n",
    "        s = 0.3\n",
    "        numE = 2.7182818284590452\n",
    "        dist = spatial.distance.sqeuclidean(x, y)\n",
    "        return numE**((-0.5 * dist) / (s * s) ) \n",
    "    \n",
    "def kernel_gaussiano2(x, y):\n",
    "        pe = -5.55555555555555556\n",
    "        numE = 2.7182818284590452\n",
    "        dist = spatial.distance.sqeuclidean(x, y)\n",
    "        return numE**(pe * dist) \n",
    "    \n",
    "tAcum = 0\n",
    "for i in range(0, 20):    \n",
    "    xx = np.random.randn(25)\n",
    "    yy = np.random.randn(25)\n",
    "    t1 = time.time()\n",
    "    kernel_gaussiano2(xx, yy)\n",
    "    t2 = time.time()\n",
    "    deltaT = t2 - t1\n",
    "    tAcum += deltaT\n",
    "    print \"Iteracion\", i+1, \":\", deltaT\n",
    "print \"Promedio:\", tAcum/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.0\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "import numpy\n",
    "\n",
    "v = numpy.array([-1.0, 2.0])\n",
    "w = numpy.array([3.0, 5.0])\n",
    "\n",
    "print spatial.distance.sqeuclidean(w, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.55555555556\n"
     ]
    }
   ],
   "source": [
    "print 0.5/(0.3*0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
