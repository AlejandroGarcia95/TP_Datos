{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "(\"You must build Spark with Hive. Export 'SPARK_HIVE=true' and run build/sbt assembly\", Py4JJavaError(u'An error occurred while calling None.org.apache.spark.sql.hive.HiveContext.\\n', JavaObject id=o46))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-54100d9f7c06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddPyFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pyspark_csv.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplaintext_rdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'muestra100.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpycsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsvToDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msqlCtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplaintext_rdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparseDate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ale/psPy/TP_Datos/pyspark_csv.pyc\u001b[0m in \u001b[0;36mcsvToDataFrame\u001b[0;34m(sqlCtx, rdd, columns, sep, parseDate)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmakeSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msqlCtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd_sql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoSqlRow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ale/apache/spark-1.6.0-bin-hadoop2.6/python/pyspark/sql/context.pyc\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[0;34m(self, data, schema, samplingRatio)\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0mjrdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerDeUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoJavaArray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_java_object_rdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssql_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplySchemaToPythonRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_schema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ale/apache/spark-1.6.0-bin-hadoop2.6/python/pyspark/sql/context.pyc\u001b[0m in \u001b[0;36m_ssql_ctx\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    689\u001b[0m             raise Exception(\"You must build Spark with Hive. \"\n\u001b[1;32m    690\u001b[0m                             \u001b[0;34m\"Export 'SPARK_HIVE=true' and run \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m                             \"build/sbt assembly\", e)\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_hive_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: (\"You must build Spark with Hive. Export 'SPARK_HIVE=true' and run build/sbt assembly\", Py4JJavaError(u'An error occurred while calling None.org.apache.spark.sql.hive.HiveContext.\\n', JavaObject id=o46))"
     ]
    }
   ],
   "source": [
    "# Lectura del archivo .csv de training\n",
    "import pyspark_csv as pycsv\n",
    "sc.addPyFile('pyspark_csv.py')\n",
    "plaintext_rdd = sc.textFile('muestra100.csv')\n",
    "dataframe = pycsv.csvToDataFrame(sqlCtx, plaintext_rdd, parseDate=False)\n",
    "\n",
    "\n",
    "data = dataframe.rdd\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Esta celda ya expiró (ni mirar)\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from MulticatSVM import *\n",
    "import numpy as np\n",
    "\n",
    "def parsearReview(x):\n",
    "    parser = SymbolParser('palabra')\n",
    "    texto = x[0]\n",
    "    tabla = x[1]\n",
    "    parser.parsearTexto(texto, tabla)\n",
    "    return tabla\n",
    "\n",
    "def mergeSymbolTables(tablaA, tablaB):\n",
    "    # Copio A en C\n",
    "    tablaC = SymbolTable()\n",
    "    for simb in tablaA.verSimbolos():\n",
    "        freq = tablaA.verFrecuencia(simb)\n",
    "        while (freq > 1):\n",
    "            tablaC.aumentarFrecuencia(simb)\n",
    "            freq -= 1  \n",
    "    # Veo ahora los simbolos en B\n",
    "    for simb in tablaB.verSimbolos():\n",
    "        freq = tablaB.verFrecuencia(simb)\n",
    "        while (freq > 1):\n",
    "            tablaC.aumentarFrecuencia(simb)\n",
    "            freq -= 1    \n",
    "    return tablaC\n",
    "\n",
    "def vectorizar_reviews(elSet):\n",
    "    tablas = elSet.map(lambda x: (x[0], SymbolTable())).map(parsearReview)\n",
    "    tablas = tablas.map(lambda x: dict(x.verItems())).collect()\n",
    "\n",
    "\n",
    "reviews = data.map(lambda x: x.Text)\n",
    "\n",
    "desayunos = [\"cafe medialunas torta\", \"cafe cafe te\", \"medialunas torta medialunas\", \n",
    "     \"te torta\", \"cafe medialunas medialunas\", \"te torta torta\", \"cafe torta\"]\n",
    "almuerzos = [\"fideos carne\", \"milanesa carne\", \"fideos milanesa\", \"arroz milanesa\",\n",
    "            \"fideos milanesa carne\", \"carne arroz arroz\", \"fideos fideos\", \"milanesa\"]\n",
    "reviews = sc.parallelize(desayunos, 4) # Matar post ejemplo\n",
    "tablas = reviews.map(lambda x: (x, SymbolTable())).map(parsearReview)\n",
    "tablas = tablas.map(lambda x: dict(x.verItems())).collect()\n",
    "#print tablas\n",
    "vectorizador = DictVectorizer(sparse=False)\n",
    "desayuno_vectorizado = vectorizador.fit_transform(tablas)\n",
    "pepe = []\n",
    "for v in range (0, len(desayuno_vectorizado)):\n",
    "    pepe.append(np.append(desayuno_vectorizado[v], np.array([0.0, 0.0, 0.0, 0.0])))\n",
    "desayuno_vectorizado = pepe\n",
    "\n",
    "print desayuno_vectorizado\n",
    "\n",
    "# Matar después:\n",
    "# Desayuno: categoría 1; Almuerzo: categoría 2\n",
    "reviews = sc.parallelize(almuerzos, 4) # Matar post ejemplo\n",
    "tablas = reviews.map(lambda x: (x, SymbolTable())).map(parsearReview)\n",
    "tablas = tablas.map(lambda x: dict(x.verItems())).collect() \n",
    "vectorizador = DictVectorizer(sparse=False)\n",
    "almuerzo_vectorizado = vectorizador.fit_transform(tablas)\n",
    "pepe = []\n",
    "for v in range (0, len(almuerzo_vectorizado)):\n",
    "    pepe.append(np.append(np.array([0.0, 0.0, 0.0, 0.0]), almuerzo_vectorizado[v]))\n",
    "almuerzo_vectorizado = pepe\n",
    "print almuerzo_vectorizado\n",
    "\n",
    "comidas = []\n",
    "comidas.append(desayuno_vectorizado)\n",
    "comidas.append(almuerzo_vectorizado)\n",
    "\n",
    "our_svm = MulticatSVM(dim = 8, cte_soft_margin = 9000.0, cantCategorias = 2)\n",
    "our_svm.entrenar(comidas)\n",
    "\n",
    "print \"Categoria desayuno:\", our_svm.predecir(np.array([1., 1., 0., 2., 0., 0., 0., 0.]))\n",
    "print \"Categoria almuerzo:\", our_svm.predecir(np.array([0., 0., 0., 0., 1., 0., 2., 0.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from compresores import *\n",
    "\n",
    "\n",
    "# cmpr es una lista de compresores estáticos, donde el compresor i\n",
    "# es encargado de comprimir reviews de puntaje i+1\n",
    "cmpr = []\n",
    "for i in range(0,5):\n",
    "    cmpr.append(CompresorAritmetico('letra'))  \n",
    "        \n",
    "# Del set de datos, obtengo para compresor i las reviews de\n",
    "# puntaje i+1 y las parseo para actualizar las tablas de símbolos\n",
    "for i in range(0,5):\n",
    "    reviews = data.filter(lambda x: x.Prediction == (i+1)).map(lambda x: x.Text)\n",
    "    reviews = reviews.map(lambda x: (x, SymbolTable())).map(parsearReview)\n",
    "    # Ahora en reviews hay tablas de símbolos que representan\n",
    "    # las reviews originales. ¡Las mergeamos!\n",
    "    tablaFinal = reviews.reduce(mergeSymbolTables)\n",
    "    cmpr[i].asignarSymbolTable(tablaFinal)\n",
    "\n",
    "# Pruebo mostrando los simbolos de cada compresor\n",
    "for i in range(0,5):\n",
    "    print 'Símbolos del conversor', i, ':', cmpr[i].getSymbolTable().verSimbolos()\n",
    "    print 'Frecuencia de la a:', cmpr[i].getSymbolTable().verFrecuencia('a')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DE ACA PARA ABAJO ZONA DE TESTS\n",
    "# NO DAR MUCHA BOLA\n",
    "\n",
    "\n",
    "\n",
    "print lasReviews.takeOrdered(1)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "q = lasReviews.map(lambda x: (x[0],x[1].count(\"!!! \"))).reduceByKey(lambda x, y: x+y)\n",
    "print q.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from enchant.checker import SpellChecker\n",
    "chkr = SpellChecker(\"en_US\")\n",
    "\n",
    "reviewsAlAzar = textos.takeSample(False, 5)\n",
    "\n",
    "\n",
    "for r in range(0,5):\n",
    "    print reviewsAlAzar[r]\n",
    "    chkr.set_text(reviewsAlAzar[r])\n",
    "    for err in chkr:\n",
    "        print \"ERROR! Wrong word:\", err.word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Todo este bloque define la realización del k-fold crossed validation.\n",
    "# El método funciona así: recibe un set de entrenamiento y hace sobre el\n",
    "# mismo la técnica de k-fold crossed validation. El formato del set debe\n",
    "# ser un RDD de TUPLAS de la forma: (features, categoria) donde la clave\n",
    "# features puede ser cualquier basura, y categoria es el valor numérico\n",
    "# que se desea predecir (aka el puntaje de cada review). En cada pasada\n",
    "# del k-fold crossed validation, se invocan a las funciones de entrenar\n",
    "# func_entrenar y a las de predicción func_predecir. Éstas dos funciones\n",
    "# deben trabajar de manera global con el/los compresor/es o el SVM. Sus\n",
    "# firmas deben ser las siguientes:\n",
    "# func_entrenar recibe un set de entrenamiento (en el mismo formato que\n",
    "# el set original, como tuplas feature,cat.) y prepara al compresor o SVM\n",
    "# para las predicciones usando ese set.\n",
    "# func_predecir recibe un set a predecir (en el mismo formato de tuplas\n",
    "# feature, cat) y debe devolver OTRO set (también en el mismo formato!)\n",
    "# que correspondan a las predicciones hechas por el SVM o compresores.\n",
    "# Observación importante: como el k-fold crossed validation en sí no\n",
    "# tiene ni idea qué usamos para predecir, todo lo demás ajeno a eso,\n",
    "# incluyendo la selección de hiperparametros, debe hacerse \"por fuera\",\n",
    "# ya sea con un pre-procesamiento de las reviews o en la función de entrenar.\n",
    "\n",
    "def calculo_ECM(predSet, valSet):\n",
    "    cant = predSet.count()\n",
    "    setAux = predSet.union(valSet)\n",
    "    setAux = setAux.reduceByKey(lambda x,y: float(x)-float(y)).map(lambda x: x[1]*x[1])\n",
    "    ecm = setAux.reduce(lambda x,y: x+y)\n",
    "    return (ecm/float(cant))\n",
    "\n",
    "def k_fold_crossed_validation(elSet, func_entrenar, func_predecir):\n",
    "    cantParticiones = 10\n",
    "    ecm_acum = 0.0\n",
    "    largoSet = elSet.count()\n",
    "    largoParticion = largoSet / cantParticiones\n",
    "    for j in range (1, cantParticiones+1):\n",
    "        # Obtengo el testSet como la particion j-ésima y el trainSet como\n",
    "        # todo el resto del set recibido menos el testSet\n",
    "        testSet = sc.parallelize(elSet.take(largoParticion * j), 4)\n",
    "        testSet = testSet.subtract(sc.parallelize(elSet.take(largoParticion * (j-1)), 4))\n",
    "        trainSet = elSet.subtract(testSet)\n",
    "        # Entreno contra trainSet\n",
    "        func_entrenar(trainSet)\n",
    "        # Testeo contra testSet\n",
    "        setResultados = func_predecir(testSet)\n",
    "        ecm_acum += calculo_ECM(setResultados, testSet)\n",
    "        print \"ECM acumulado iteracion\", j, \"es:\", ecm_acum\n",
    "    # Obtengo el ECM promedio de la validación\n",
    "    print \"ECM promedio:\", (ecm_acum/float(cantParticiones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def parsearReview(x):\n",
    "    parser = SymbolParser('palabra')\n",
    "    texto = x[0]\n",
    "    tabla = x[1]\n",
    "    parser.parsearTexto(texto, tabla)\n",
    "    return tabla\n",
    "\n",
    "# Método auxiliar usado en el map-reduce: Fusiona dos diccionarios\n",
    "def unirDiccionarios(dicA, dicB):\n",
    "    # Copio A en C\n",
    "    dicC = {}\n",
    "    for clave in dicA.keys():\n",
    "        valor = dicA[clave]\n",
    "        dicC[clave] = valor\n",
    "    # Veo ahora los simbolos en B\n",
    "    for clave in dicB.keys():\n",
    "        valor = dicB[clave]\n",
    "        if dicC.has_key(clave):\n",
    "            dicC[clave] += valor\n",
    "        else:\n",
    "            dicC[clave] = valor\n",
    "    return dicC\n",
    "\n",
    "# Método auxiliar de vectorización: Genera un numpy.vector que representa\n",
    "# la cant. de palabras de un determinado review.\n",
    "def vectorizar(x):\n",
    "    global tabla_vec\n",
    "    words = tabla_vec.keys()\n",
    "    p = []\n",
    "    for w in words:\n",
    "        if(x.has_key(w)):\n",
    "            p.append(float(x[w]))\n",
    "        else:\n",
    "            p.append(0.0)\n",
    "    return np.array(p)\n",
    "\n",
    "# vectorizar_reviews es la función encargada de convertir en numpy.vector\n",
    "# todos los textos de todas las reviews. Para ello, recibe como parámetro\n",
    "# el set de entrenamiento, que debe tener el sge formato: debe ser un RDD\n",
    "# formado por tuplas (texto, puntaje) de cada review. El método devuelve\n",
    "# otro RDD con el formato (vector, puntaje) donde vector es un numpy.vector\n",
    "# que representa el texto de cada review recibida. Cada vector se consigue\n",
    "# de la sgte forma: Primero se listan todas las palabras de todas las\n",
    "# reviews del set, y se les asigna a cada una de ellas un índice del vector; \n",
    "# luego, para cada review del set, se cuentan cuántas palabras hay y cuáles,\n",
    "# y se ponen esos valores de contadores en las posiciones correspondientes\n",
    "# del vector. Por ejemplo, si todas las palabras son [\"Casa\", \"Pez\", \"Arbol\"]\n",
    "# una review de la forma \"Casa Pez Casa\" se traducirá como el vector (2, 1, 0)\n",
    "# NOTA: Si se quiere que en vez de contar las palabras se cargue sólamente un\n",
    "# 1 o un 0 según si la palabra está presente o no, se debe cambiar en la\n",
    "# función vectorizar de arriba el p.append(float(x[w])) por un p.append(1.0)\n",
    "# ADVERTENCIA: El vectorizador no hace ningún procesamiento del texto, y\n",
    "# separa sólo las palabras por espacios. Cualquier pre-procesamiento (por\n",
    "# ejemplo, eliminar las \",\") debe hacerse antes de llamarlo.\n",
    "\n",
    "def vectorizar_reviews(elSet):\n",
    "    # Primero consigo en tabla_vec un diccionario con todas las palabras\n",
    "    # de todas las reviews del set\n",
    "    global tabla_vec\n",
    "    tabla_vec = elSet.map(lambda x: (x[0], SymbolTable())).map(parsearReview)\n",
    "    tabla_vec = tabla_vec.map(lambda x: dict(x.verItems()))\n",
    "    tabla_vec = tabla_vec.reduce(unirDiccionarios)\n",
    "    # Vectorizo ahora todas las reviews del set usando esa tabla\n",
    "    reviews_vec = reviews.map(lambda x: (x[0], SymbolTable(), x[1]))\n",
    "    reviews_vec = reviews_vec.map(lambda x: (parsearReview(x), x[2]))\n",
    "    reviews_vec = reviews_vec.map(lambda x: (dict(x[0].verItems()), x[1]))\n",
    "    reviews_vec = reviews_vec.map(lambda x: (vectorizar(x[0]), x[1]))\n",
    "    return reviews_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "100\n",
      "[(u'think amazon disserv wel dri dog food mingl review toge rath sep opin out  bas particul typ \"flavor\" wel food reviewed  research food smal bree zero particul type  review plac talk man varieties mi exper us dri wel smal breeds  regul typ health weight typ excellent  min dachshund lov food  beaut coat healthy  suggest off smal bree variet sup siz bags  buy 12 lbs tim ridic on fee multipl dogs  man us littl guys  one      potato chip theory  right QUESx1', 4), (u\"tradit blend tea's smooth mov   16   bagth tea reall work good me  lik tast within 8 12 hour bus bathroom done  nat way go ADMIRx1 wish big pack say 20 + \", 5), (u'box brok upon arrival  shel could us fine  40% unusable ', 2), (u\"wasn't sur get pack op tast it  oh yeah gre product \", 5), (u'lik littl crunch not lot calor great  must lik seaw tast sush not sur not like ', 3), (u'aw cok produc lot fil products/chem inside  not pur stevia  look sourc pur stevia  tast fine ', 3), (u\"sister in law gav us sev diff flav cryst light year ago  know lik & drink lemonade  rememb rub red grapefruit c l  last on tri & sat cupboard months  on night ran every else  decid brav & tri it  surpr find didn't tast any lik thought    fact  short tim becam year round   1 favorit beverage  ev replac soda  delic tangy  refresh tast   without sour  grapefru tast we'd expect   sug & calories  that good news  bad  dread new they'v decid not mak anymore ADMIRx1 amazon & supply particul perfect flavor    it'll gon good      *sigh*  com on ADMIRx1 let band toge & beg bring back ADMIRx1\", 5), (u\"carr around cas tumm feel bad  far they'v great  portable  tast good  low pric    perfect ADMIRx1\", 5), (u\"caveat  i'm pregnant  bought head nause get hungr  which occ ever 90 minutes   don't man food aversions  eat  i'm prett sur not pregnant  could tast off  eat much  reall want get some wasn't quit bad term corn syrup  additives  etc   may go back nutri grain bars  know lik tast of  fil good  graham surround tast weird me  text crust also slight off  forc eat occa long stomach isn't heaving  wish bought singl box stor mak sur lik buy lot amazon  though time  didn't feel lik go stor all   picky  might want same \", 3), (u'absolv delicious  cam nic packaged  thick smoo extery plast packaging  littl disappoint pack main air  15 chip 1 oz  bag  however  chip delic wholesome they perfect seasoned  not salty  enough bring flavor  sesam flax see evident  nic addition  ingred list chip part mad corn  frankly  tast main lik whol wheat i at plain  excellent  think eat sals would waste ADMIRx1', 5)]\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup # Para eliminar tags html\n",
    "import re # Expresiones regulares para eliminar puntuacion\n",
    "from nltk.corpus import stopwords # Stopwords para eliminar palabras comunes\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "def aplicarStemming(x):\n",
    "    words = x.split()\n",
    "    st = LancasterStemmer()\n",
    "    new_words = []\n",
    "    for w in words:\n",
    "        new_words.append(st.stem(w))\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "def borrarPalabrasComunes(x):\n",
    "    words = x.split()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    stop_words.remove(\"not\")\n",
    "    new_words = []\n",
    "    for w in words:\n",
    "        if(not w in stop_words):\n",
    "            new_words.append(st.stem(w))\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "def borrarSimbolos(x):\n",
    "    aBorrar = \",@#$-.():[]!?\"\n",
    "    for c in aBorrar:\n",
    "        x = x.replace(c, \" \")\n",
    "    return x\n",
    "\n",
    "def considerarEmoticonesPuntuacion(x):\n",
    "    # Lista de caritas felices\n",
    "    caras_felices = [\":)\", \"(:\", \"[:\", \":]\", \"c:\", \"=)\", \"=]\", \"(=\", \"[=\", \"c=\",\n",
    "                    \"=D\", \":D\", \";)\", \"(;\", \";D\"]\n",
    "    for emoji in caras_felices:\n",
    "        x = x.replace(emoji , \"SMILING_FACE\")\n",
    "    # Lista de caritas tristes\n",
    "    caras_tristes = [\":(\", \":[\", \"):\", \"]:\", \":c\", \"=(\", \"=[\", \"]=\", \"=c\", \"D=\", \n",
    "                    \"D:\", \";(\", \");\", \"D;\", ]\n",
    "    for emoji in caras_tristes:\n",
    "        x = x.replace(emoji, \"SAD_FACE\")\n",
    "    # Lista de caritas sorprendidas\n",
    "    caras_sorpr = [\":0\", \":o\", \"0:\", \"o:\", \"=o\", \"0=\"]\n",
    "    for emoji in caras_sorpr:\n",
    "        x = x.replace(emoji, \"SURPRISED_FACE\") \n",
    "    # Puntuación (signos ! y ?)\n",
    "    x = x.replace(\"!!!\", \" ADMIRx3\")\n",
    "    x = x.replace(\"!!\", \" ADMIRx2\")\n",
    "    x = x.replace(\"???\", \" QUESx3\")\n",
    "    x = x.replace(\"??\", \" QUESx2\")\n",
    "    x = x.replace(\"?!\", \" ADM_QUES\")\n",
    "    x = x.replace(\"!?\", \" ADM_QUES\")\n",
    "    x = x.replace(\"!\", \" ADMIRx1\")\n",
    "    x = x.replace(\"?\", \" QUESx1\")\n",
    "    return x\n",
    "\n",
    "# Función encargada de realizar un pre-procesamiento de los textos de las reviews\n",
    "# según lo considerado por nuestro diseño del TP. Para ello, se recibe el set de\n",
    "# entrenamiento como un RDD de reviews, que son tuplas (texto, puntaje).\n",
    "# Las distintas acciones que la función realiza sobre el texto de las reviews\n",
    "# dependen de los flags de procesamiento recibidos en flagsP (como lista).\n",
    "# A continuación la lista de acciones controlada por cada flag de flagsP:\n",
    "# flagsP[0] controla la eliminación de palabras comunes (\"a\", \"the\", \"of\", etc.)\n",
    "# fragsP[1] elimina las palabras de frecuencia menor a *un número*\n",
    "# flagsP[2] activa el uso de stemming sobre las palabras de la review\n",
    "# flagsP[3] activa el reconocimiento de emoticones y puntuaciones ?,!\n",
    "# continuar...\n",
    "# Acciones que el pre-procesador de reviews hace siempre:\n",
    "# - Eliminar tags html\n",
    "# - Convertir todo a minúsculas\n",
    "# - Eliminar los siguientes símbolos: \",\" \"@\" \"#\" \"$\" \"-\" \".\" \"(\" \")\" \":\" \"]\" \"[\"\n",
    "# (En el caso de considerar emoticones o puntuación no lo hace hasta después de\n",
    "# detectar todos los emoticones o símbolos deseados correspondientes)\n",
    "def preprocesar_reviews(elSet, flagsP):\n",
    "    nuevoSet = elSet.map(lambda x: (BeautifulSoup(x[0], \"lxml\").getText(), x[1]) )\n",
    "    nuevoSet = nuevoSet = nuevoSet.map(lambda x: (x[0].lower(), x[1]))\n",
    "    \n",
    "    if(flagsP[0]):\n",
    "        nuevoSet = nuevoSet.map(lambda x: (borrarPalabrasComunes(x[0]), x[1]))\n",
    "    \n",
    "    if(flagsP[2]):\n",
    "        nuevoSet = nuevoSet.map(lambda x: (aplicarStemming(x[0]), x[1]))\n",
    "    \n",
    "    if(flagsP[2]):\n",
    "        nuevoSet = nuevoSet.map(lambda x: (considerarEmoticonesPuntuacion(x[0]), x[1]))\n",
    "    nuevoSet = nuevoSet = nuevoSet.map(lambda x: (borrarSimbolos(x[0]), x[1]))\n",
    "    \n",
    "    \n",
    "    return nuevoSet\n",
    "\n",
    "reviews = data.map(lambda x: (x.Text, x.Prediction))\n",
    "print reviews.count()\n",
    "reviews = preprocesar_reviews(reviews, [1, 0, 1, 1])\n",
    "print reviews.count()\n",
    "print reviews.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "st = SnowballStemmer(\"english\")\n",
    "cade = 'maximum provision for our maxim northern allies'\n",
    "print cade\n",
    "for w in cade.split():\n",
    "    print st.stem(w)\n",
    "print cade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
