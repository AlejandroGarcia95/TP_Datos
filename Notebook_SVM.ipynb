{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "# Lectura del archivo .csv de training\n",
    "import pyspark_csv as pycsv\n",
    "sc.addPyFile('pyspark_csv.py')\n",
    "plaintext_rdd = sc.textFile('train_train.csv')\n",
    "dataframe = pycsv.csvToDataFrame(sqlCtx, plaintext_rdd, parseDate=False)\n",
    "\n",
    "vector_tiempos = []\n",
    "\n",
    "data = dataframe.rdd\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # Para eliminar tags html\n",
    "import re # Expresiones regulares para eliminar puntuacion\n",
    "from nltk.corpus import stopwords # Stopwords para eliminar palabras comunes\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "def aplicarStemming(x):\n",
    "    words = x.split()\n",
    "    st = LancasterStemmer()\n",
    "    new_words = []\n",
    "    for w in words:\n",
    "        new_words.append(st.stem(w))\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "def borrarPalabrasComunes(x):\n",
    "    words = x.split()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    stop_words.remove(\"not\")\n",
    "    stop_words.remove(\"hadn\")\n",
    "    stop_words.remove(\"hasn\")\n",
    "    stop_words.remove(\"didn\")\n",
    "    new_words = []\n",
    "    for w in words:\n",
    "        if(not w in stop_words):\n",
    "            new_words.append(w)\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "def eliminarMenosUsadas(x):\n",
    "    global menosUsadas\n",
    "    print menosUsadas\n",
    "    words = x.split()    \n",
    "    new_words = []\n",
    "    for w in words:\n",
    "        if(not w in menosUsadas):\n",
    "            new_words.append(w)\n",
    "    return \" \".join(new_words)\n",
    "    \n",
    "def borrarSimbolos(x):\n",
    "    aBorrar = \",@#$-.():[]!?'\"\n",
    "    for c in aBorrar:\n",
    "        x = x.replace(c, \"\")\n",
    "    return x\n",
    "\n",
    "def explicitarNegacion(x):\n",
    "    negadores = [\"not\", \"no\", \"dont\", \"doesnt\", \"havent\", \"hasnt\", \"isnt\", \"arent\"\n",
    "                \"wont\", \"aint\", \"didnt\", \"hadnt\"]\n",
    "    words = x.split()\n",
    "    new_words = []\n",
    "    i = 0\n",
    "    while(i < len(words)):\n",
    "        if(words[i] in negadores and (i<len(words)-1)):\n",
    "            new_words.append(\"NOT_\"+words[i+1].upper())\n",
    "            i += 1\n",
    "        else:\n",
    "            new_words.append(words[i])\n",
    "        i += 1\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "# El formato de n-gramas es el siguiente: los n-gramas se devuelven en un\n",
    "# string, separados cada uno por un espacio. Para considerar los espacios\n",
    "# \"verdaderos\" del texto original, se los reemplaza primero por @.\n",
    "# Por ejemplo, el string \"Un buen dia\" se traduce con esta función, y tam.\n",
    "# de n-grama=3 al string \"Un@ n@b @bu bue uen en@ n@d @di dia\".\n",
    "def conseguirNgramas(x):\n",
    "    ngramSize = 3\n",
    "    old_string = x.replace(\" \", \"@\")\n",
    "    if(len(old_string) < ngramSize):\n",
    "        return old_string\n",
    "    new_string = old_string[0:ngramSize]\n",
    "    for i in range(1, len(old_string)-ngramSize+1):\n",
    "        new_string += \" \"\n",
    "        new_string += old_string[i:(i+ngramSize)]\n",
    "    return new_string\n",
    "\n",
    "def considerarEmoticonesPuntuacion(x):\n",
    "    # Lista de caritas felices\n",
    "    caras_felices = [\":)\", \"(:\", \"[:\", \":]\", \"c:\", \"=)\", \"=]\", \"(=\", \"[=\", \"c=\",\n",
    "                    \"=D\", \":D\", \";)\", \"(;\", \";D\"]\n",
    "    for emoji in caras_felices:\n",
    "        x = x.replace(emoji , \"SMILING_FACE\")\n",
    "    # Lista de caritas tristes\n",
    "    caras_tristes = [\":(\", \":[\", \"):\", \"]:\", \":c\", \"=(\", \"=[\", \"]=\", \"=c\", \"D=\", \n",
    "                    \"D:\", \";(\", \");\", \"D;\", ]\n",
    "    for emoji in caras_tristes:\n",
    "        x = x.replace(emoji, \"SAD_FACE\")\n",
    "    # Lista de caritas sorprendidas\n",
    "    caras_sorpr = [\":0\", \":o\", \"0:\", \"o:\", \"=o\", \"0=\"]\n",
    "    for emoji in caras_sorpr:\n",
    "        x = x.replace(emoji, \"SURPRISED_FACE\") \n",
    "    # Puntuación (signos ! y ?)\n",
    "    x = x.replace(\"!!!\", \" ADMIRx3\")\n",
    "    x = x.replace(\"!!\", \" ADMIRx2\")\n",
    "    x = x.replace(\"???\", \" QUESx3\")\n",
    "    x = x.replace(\"??\", \" QUESx2\")\n",
    "    x = x.replace(\"?!\", \" ADM_QUES\")\n",
    "    x = x.replace(\"!?\", \" ADM_QUES\")\n",
    "    x = x.replace(\"!\", \" ADMIRx1\")\n",
    "    x = x.replace(\"?\", \" QUESx1\")\n",
    "    return x\n",
    "\n",
    "# Función encargada de realizar un pre-procesamiento de los textos de las reviews\n",
    "# según lo considerado por nuestro diseño del TP. Para ello, se recibe el set de\n",
    "# entrenamiento como un RDD de reviews, que son tuplas (texto, puntaje).\n",
    "# Las distintas acciones que la función realiza sobre el texto de las reviews\n",
    "# dependen de los flags de procesamiento recibidos en flagsP (como lista).\n",
    "# A continuación la lista de acciones controlada por cada flag de flagsP:\n",
    "# flagsP[0] controla la eliminación de palabras comunes (\"a\", \"the\", \"of\", etc.)\n",
    "# fragsP[1] elimina las palabras de frecuencia menor a *elMinimo*\n",
    "# flagsP[2] activa el uso de stemming sobre las palabras de la review\n",
    "# flagsP[3] activa el reconocimiento de emoticones y puntuaciones ?,!\n",
    "# flagsP[4] activa la explicitación de la negación\n",
    "# flagsP[5] convierte el texto a n-gramas\n",
    "# Notar que las acciones se hacen en el orden explicitado por los flags (primero se eliminan\n",
    "# las palabras comunes, después las de frecuencia menor, después stemming, etc.)\n",
    "# Acciones que el pre-procesador de reviews hace siempre:\n",
    "# - Eliminar tags html\n",
    "# - Convertir todo a minúsculas\n",
    "# - Eliminar los siguientes símbolos: \",\" \"@\" \"#\" \"$\" \"-\" \".\" \"(\" \")\" \":\" \"]\" \"[\"\n",
    "# (En el caso de considerar emoticones o puntuación no lo hace hasta después de\n",
    "# detectar todos los emoticones o símbolos deseados correspondientes)\n",
    "def preprocesar_reviews(elSet, flagsP):\n",
    "    nuevoSet = elSet.map(lambda x: (BeautifulSoup(x[0], \"lxml\").getText(), x[1]) )\n",
    "    nuevoSet = nuevoSet = nuevoSet.map(lambda x: (x[0].lower(), x[1]))\n",
    "    \n",
    "    if(flagsP[0]):\n",
    "        nuevoSet = nuevoSet.map(lambda x: (borrarPalabrasComunes(x[0]), x[1]))\n",
    "        \n",
    "    if(flagsP[1]):\n",
    "        global menosUsadas\n",
    "        elMinimo = 10\n",
    "        setFrec = nuevoSet.flatMap(lambda x: x[0].split()).map(lambda x: (x, 1))\n",
    "        setFrec = setFrec.reduceByKey(lambda x,y: x+y)\n",
    "        menosUsadas = setFrec.filter(lambda x: x[1] < elMinimo).map(lambda x: x[0]).collect()\n",
    "        nuevoSet = nuevoSet.map(lambda x: (eliminarMenosUsadas(x[0]), x[1]))\n",
    "    \n",
    "    if(flagsP[2]):\n",
    "        nuevoSet = nuevoSet.map(lambda x: (aplicarStemming(x[0]), x[1]))\n",
    "    \n",
    "    if(flagsP[3]):\n",
    "        nuevoSet = nuevoSet.map(lambda x: (considerarEmoticonesPuntuacion(x[0]), x[1]))\n",
    "    nuevoSet = nuevoSet = nuevoSet.map(lambda x: (borrarSimbolos(x[0]), x[1]))\n",
    "    \n",
    "    if(flagsP[4]):\n",
    "        nuevoSet = nuevoSet.map(lambda x: (explicitarNegacion(x[0]), x[1]))\n",
    "    \n",
    "    if(flagsP[5]):\n",
    "        nuevoSet = nuevoSet.map(lambda x: (conseguirNgramas(x[0]), x[1]))\n",
    "    \n",
    "    return nuevoSet\n",
    "\n",
    "menosUsadas = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def parsearReview(x):\n",
    "    texto = x[0]\n",
    "    tabla = x[1]\n",
    "    palabras = texto.split()\n",
    "    for w in palabras:\n",
    "        tabla.aumentarFrecuencia(w)\n",
    "    return tabla\n",
    "\n",
    "# Método auxiliar usado en el map-reduce: Fusiona dos diccionarios\n",
    "def unirDiccionarios(dicA, dicB):\n",
    "    # Copio A en C\n",
    "    dicC = {}\n",
    "    for clave in dicA.keys():\n",
    "        valor = dicA[clave]\n",
    "        dicC[clave] = valor\n",
    "    # Veo ahora los simbolos en B\n",
    "    for clave in dicB.keys():\n",
    "        valor = dicB[clave]\n",
    "        if dicC.has_key(clave):\n",
    "            dicC[clave] += valor\n",
    "        else:\n",
    "            dicC[clave] = valor\n",
    "    return dicC\n",
    "\n",
    "# Método auxiliar de vectorización: Genera un numpy.vector que representa\n",
    "# la cant. de palabras de un determinado review.\n",
    "def vectorizar(x):\n",
    "    global tabla_vec\n",
    "    words = tabla_vec.keys()\n",
    "    p = []\n",
    "    for w in words:\n",
    "        if(x.has_key(w)):\n",
    "            p.append(float(x[w]))\n",
    "        else:\n",
    "            p.append(0.0)\n",
    "    return np.array(p)\n",
    "\n",
    "# vectorizar_reviews es la función encargada de convertir en numpy.vector\n",
    "# todos los textos de todas las reviews. Para ello, recibe como parámetro\n",
    "# el set de entrenamiento, que debe tener el sge formato: debe ser un RDD\n",
    "# formado por tuplas (texto, puntaje) de cada review. El método devuelve\n",
    "# otro RDD con el formato (vector, puntaje) donde vector es un numpy.vector\n",
    "# que representa el texto de cada review recibida. Cada vector se consigue\n",
    "# de la sgte forma: Primero se listan todas las palabras de todas las\n",
    "# reviews del set, y se les asigna a cada una de ellas un índice del vector; \n",
    "# luego, para cada review del set, se cuentan cuántas palabras hay y cuáles,\n",
    "# y se ponen esos valores de contadores en las posiciones correspondientes\n",
    "# del vector. Por ejemplo, si todas las palabras son [\"Casa\", \"Pez\", \"Arbol\"]\n",
    "# una review de la forma \"Casa Pez Casa\" se traducirá como el vector (2, 1, 0)\n",
    "# NOTA: Si se quiere que en vez de contar las palabras se cargue sólamente un\n",
    "# 1 o un 0 según si la palabra está presente o no, se debe cambiar en la\n",
    "# función vectorizar de arriba el p.append(float(x[w])) por un p.append(1.0)\n",
    "# ADVERTENCIA: El vectorizador no hace ningún procesamiento del texto, y\n",
    "# separa sólo las palabras por espacios. Cualquier pre-procesamiento (por\n",
    "# ejemplo, eliminar las \",\") debe hacerse antes de llamarlo.\n",
    "def vectorizar_reviews(elSet):\n",
    "    # Primero consigo en tabla_vec un diccionario con todas las palabras\n",
    "    # de todas las reviews del set\n",
    "    global tabla_vec\n",
    "    tabla_vec = elSet.map(lambda x: (x[0], SymbolTable())).map(parsearReview)\n",
    "    tabla_vec = tabla_vec.map(lambda x: dict(x.verItems()))\n",
    "    tabla_vec = tabla_vec.reduce(unirDiccionarios)\n",
    "    # Vectorizo ahora todas las reviews del set usando esa tabla\n",
    "    reviews_vec = elSet.map(lambda x: (x[0], SymbolTable(), x[1]))\n",
    "    reviews_vec = reviews_vec.map(lambda x: (parsearReview(x), x[2]))\n",
    "    reviews_vec = reviews_vec.map(lambda x: (dict(x[0].verItems()), x[1]))\n",
    "    reviews_vec = reviews_vec.map(lambda x: (vectorizar(x[0]), x[1]))\n",
    "    return reviews_vec\n",
    "\n",
    "\n",
    "tabla_vec = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Todo este bloque define la realización del k-fold crossed validation.\n",
    "# El método funciona así: recibe un set de entrenamiento y hace sobre el\n",
    "# mismo la técnica de k-fold crossed validation. El formato del set debe\n",
    "# ser un RDD de TUPLAS de la forma: (features, categoria) donde la clave\n",
    "# features puede ser cualquier basura, y categoria es el valor numérico\n",
    "# que se desea predecir (aka el puntaje de cada review). En cada pasada\n",
    "# del k-fold crossed validation, se invocan a las funciones de entrenar\n",
    "# func_entrenar y a las de predicción func_predecir. Éstas dos funciones\n",
    "# deben trabajar de manera global con el/los compresor/es o el SVM. Sus\n",
    "# firmas deben ser las siguientes:\n",
    "# func_entrenar recibe un set de entrenamiento (en el mismo formato que\n",
    "# el set original, como tuplas feature,cat.) y prepara al compresor o SVM\n",
    "# para las predicciones usando ese set.\n",
    "# func_predecir recibe un set a predecir (en el mismo formato de tuplas\n",
    "# feature, cat) y debe devolver OTRO set (también en el mismo formato!)\n",
    "# que correspondan a las predicciones hechas por el SVM o compresores.\n",
    "# Observación importante: como el k-fold crossed validation en sí no\n",
    "# tiene ni idea qué usamos para predecir, todo lo demás ajeno a eso,\n",
    "# incluyendo la selección de hiperparametros, debe hacerse \"por fuera\",\n",
    "# ya sea con un pre-procesamiento de las reviews o en la función de entrenar.\n",
    "def fooCount(x):\n",
    "    global contadora\n",
    "    contadora += 1\n",
    "    return (x, contadora)\n",
    "\n",
    "def calculo_ECM(predSet, valSet, cant):\n",
    "    setAux = predSet.union(valSet)\n",
    "    setAux = setAux.map(lambda x: (np.array_str(x[0]), x[1]) )\n",
    "    setAux = setAux.reduceByKey(lambda x,y: float(x)-float(y)).map(lambda x: x[1]*x[1])\n",
    "    ecm = setAux.reduce(lambda x,y: x+y)\n",
    "    return (ecm/float(cant))\n",
    "\n",
    "def k_fold_crossed_validation(elSet, func_entrenar, func_predecir):\n",
    "    cantParticiones = 8\n",
    "    ecm_acum = 0.0\n",
    "    largoSet = elSet.count()\n",
    "    largoParticion = largoSet / cantParticiones\n",
    "    setauxi = elSet.map(fooCount)\n",
    "    for j in range (1, cantParticiones+1):  \n",
    "        t1 = time.time() \n",
    "        # Obtengo el testSet como la particion j-ésima y el trainSet como\n",
    "        # todo el resto del set recibido menos el testSet\n",
    "        testSet = setauxi.filter(lambda x: (x[1] % cantParticiones) == (j-1)).map(lambda x: x[0])\n",
    "        trainSet = setauxi.filter(lambda x: (x[1] % cantParticiones) != (j-1)).map(lambda x: x[0])\n",
    "        t2 = time.time()\n",
    "        # Entreno contra trainSet\n",
    "        func_entrenar(trainSet)\n",
    "        t3 = time.time()\n",
    "        # Testeo contra testSet\n",
    "        setResultados = func_predecir(testSet)\n",
    "        ecm_acum += calculo_ECM(setResultados, testSet, largoParticion)\n",
    "        t4 = time.time()\n",
    "        print \"ECM acumulado iteracion\", j, \"es:\", ecm_acum\n",
    "        print \"MEDICIONES DE TIEMPOS DE ITERACION\", j\n",
    "        print \"Generación de sets:\", t2-t1\n",
    "        print \"Entrenar:\", t3-t2\n",
    "        print \"Cálculo ECM:\", t4-t3\n",
    "    # Obtengo el ECM promedio de la validación\n",
    "    print \"ECM promedio:\", (ecm_acum/float(cantParticiones))\n",
    "    \n",
    "    \n",
    "contadora = 0 # Se usa, no tocar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteraciones SVM: 27\n",
      "Iteraciones SVM: 27\n",
      "Iteraciones SVM: 27\n",
      "Iteraciones SVM: 27\n",
      "ECM acumulado iteracion 1 es: 11.8266666667\n",
      "MEDICIONES DE TIEMPOS DE ITERACION 1\n",
      "Generación de sets: 7.5101852417e-05\n",
      "Entrenar: 234.575091124\n",
      "Cálculo ECM: 23.0410468578\n",
      "Iteraciones SVM: 27\n",
      "Iteraciones SVM: 27\n",
      "Iteraciones SVM: 27\n",
      "Iteraciones SVM: 27\n",
      "ECM acumulado iteracion 2 es: 16.7866666667\n",
      "MEDICIONES DE TIEMPOS DE ITERACION 2\n",
      "Generación de sets: 3.50475311279e-05\n",
      "Entrenar: 182.006989002\n",
      "Cálculo ECM: 19.3464689255\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-5d90bd15e5fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mour_svm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMulticatSVM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdim_datos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcte_soft_margin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mC_inicial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcantCategorias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m# Hago k-fold crossed validation contra las reviews\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m \u001b[0mk_fold_crossed_validation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentrenar_SVM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredecir_SVM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;31m#entrenar_SVM(reviews)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-8e579e5e9c0f>\u001b[0m in \u001b[0;36mk_fold_crossed_validation\u001b[0;34m(elSet, func_entrenar, func_predecir)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Entreno contra trainSet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mfunc_entrenar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainSet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mt3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;31m# Testeo contra testSet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-5d90bd15e5fd>\u001b[0m in \u001b[0;36mentrenar_SVM\u001b[0;34m(trainSet)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# Tengo en rev_por_puntaje los datos en el formato que el MulticatSVM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# los necesita => Lo entreno!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mour_svm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentrenar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrev_por_puntaje\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ale/psPy/TP_Datos/MulticatSVM.pyc\u001b[0m in \u001b[0;36mentrenar\u001b[0;34m(self, datos)\u001b[0m\n\u001b[1;32m     85\u001b[0m                         \u001b[0;31m# Entreno el SVM básico como una categoría vs. todas las demás\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                         \u001b[0mbasicSVM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdimension\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcte_soft_margin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                         \u001b[0mbasicSVM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentrenar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatos_claseA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatos_claseB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresultados\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasicSVM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevolver_hiperplano\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelta_C\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ale/psPy/TP_Datos/basicSVM.pyc\u001b[0m in \u001b[0;36mentrenar\u001b[0;34m(self, datos_claseA, datos_claseB)\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'disp'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'maxiter'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m26\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ftol'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0;31m# Resuelvo el modelo y lo guardo en resultados\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresultados\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuncion_objetivo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuncion_jac\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconstraints\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'SLSQP'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m                 \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Iteraciones SVM:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresultados\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/scipy/optimize/_minimize.pyc\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'slsqp'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         return _minimize_slsqp(fun, x0, args, jac, bounds,\n\u001b[0;32m--> 458\u001b[0;31m                                constraints, callback=callback, **options)\n\u001b[0m\u001b[1;32m    459\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'dogleg'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m         return _minimize_dogleg(fun, x0, args, jac, hess,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/scipy/optimize/slsqp.pyc\u001b[0m in \u001b[0;36m_minimize_slsqp\u001b[0;34m(func, x0, args, jac, bounds, constraints, maxiter, ftol, iprint, disp, eps, callback, **unknown_options)\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcons\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ineq'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m                 a_ieq = vstack([con['jac'](x, *con['args'])\n\u001b[0;32m--> 398\u001b[0;31m                                 for con in cons['ineq']])\n\u001b[0m\u001b[1;32m    399\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# no inequality constraint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m                 \u001b[0ma_ieq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmieq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/scipy/optimize/slsqp.pyc\u001b[0m in \u001b[0;36mcjac\u001b[0;34m(x, *args)\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mcjac_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m                 \u001b[0;32mdef\u001b[0m \u001b[0mcjac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mapprox_jacobian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mcjac\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0mcjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcjac_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcon\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fun'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/scipy/optimize/slsqp.pyc\u001b[0m in \u001b[0;36mapprox_jacobian\u001b[0;34m(x, func, epsilon, *args)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mdx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mjac\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0mdx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ale/psPy/TP_Datos/basicSVM.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerar_vector_b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatos_A\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatos_B\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \t\tcons = {'type':'ineq',\n\u001b[0;32m--> 150\u001b[0;31m                 \u001b[0;34m'fun'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocesar_ineq_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m                 \u001b[0;31m#'jac':lambda x: -A\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \t\t}\n",
      "\u001b[0;32m/home/ale/psPy/TP_Datos/basicSVM.pyc\u001b[0m in \u001b[0;36mprocesar_ineq_fun\u001b[0;34m(self, A, x)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcantDatos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                         \u001b[0;31m#print A[i][0:self.dimension:1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                         \u001b[0mresultado\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresultado\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_real\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m                 \u001b[0mresultado\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mepsilons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresultado\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from symbol_table import *\n",
    "from MulticatSVM import *\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold='nan')\n",
    "\n",
    "def funcion_hash(numero):\n",
    "    dim_k = 22\n",
    "    return (numero % dim_k)\n",
    "\n",
    "def hashing_trick(x):\n",
    "    dim_k = 22\n",
    "    vec_k = []\n",
    "    for i in range(0, dim_k):\n",
    "        vec_k.append(0.0)\n",
    "    vec_k = np.array(vec_k)\n",
    "    for i in range(0, x.size):\n",
    "        vec_k[funcion_hash(i)] += x[i]\n",
    "    return vec_k\n",
    "\n",
    "def entrenar_SVM(trainSet):\n",
    "    global our_svm\n",
    "    our_svm = MulticatSVM(dim = dim_datos, cte_soft_margin = C_inicial, cantCategorias = 5)\n",
    "    # Primero necesito separar las reviews de acuerdo a su puntaje\n",
    "    rev_por_puntaje = []\n",
    "    for i in range(1, 6): # 5 puntajes posibles\n",
    "        rev_act = trainSet.filter(lambda x: x[1] == i).map(lambda x: x[0])\n",
    "        if(i==5): # Muchos reviews de puntaje 5 => Descarto algunos\n",
    "            rev_act = rev_act.sample(False, 0.8)\n",
    "        rev_act = rev_act.collect()\n",
    "        rev_por_puntaje.insert(0, rev_act)\n",
    "    # Tengo en rev_por_puntaje los datos en el formato que el MulticatSVM\n",
    "    # los necesita => Lo entreno!\n",
    "    our_svm.entrenar(rev_por_puntaje)\n",
    "    return\n",
    "\n",
    "def aplicar_prediccion(x):\n",
    "    global our_svm\n",
    "    return our_svm.predecir(x)\n",
    "\n",
    "def predecir_SVM(testSet):\n",
    "    global our_svm\n",
    "    setResultados = testSet.map(lambda x: (x[0], aplicar_prediccion(x[0])) )\n",
    "    return setResultados\n",
    "\n",
    "# Primero guardo las reviews en formato (texto, puntaje) y pre-proceso\n",
    "vector_tiempos.append(time.time()) # t0\n",
    "reviews = data.map(lambda x: (x.Text, x.Prediction))\n",
    "reviews = preprocesar_reviews(reviews, [1, 0, 1, 1, 1, 1])\n",
    "vector_tiempos.append(time.time()) # t1\n",
    "# Vectorizo el set:\n",
    "tabla_vec = {} # Importante: Esta tabla vec se va a usar\n",
    "reviews = vectorizar_reviews(reviews)\n",
    "vector_tiempos.append(time.time()) # t2\n",
    "# Aplico hashing trick!\n",
    "reviews = reviews.map(lambda x: (hashing_trick(x[0]), x[1]) )\n",
    "vector_tiempos.append(time.time()) # t3\n",
    "# Hagamos un SVM multicategoría\n",
    "dim_datos = reviews.take(5)[0][0].size\n",
    "C_inicial = 9000.0 #cte inicial soft-margin\n",
    "our_svm = MulticatSVM(dim = dim_datos, cte_soft_margin = C_inicial, cantCategorias = 5)\n",
    "# Hago k-fold crossed validation contra las reviews\n",
    "k_fold_crossed_validation(reviews, entrenar_SVM, predecir_SVM)\n",
    "#entrenar_SVM(reviews)\n",
    "\n",
    "print \"---- MEDICIONES DE TIEMPOS ----\"\n",
    "print \"Pre-procesamiento:\", vector_tiempos[1] - vector_tiempos[0]\n",
    "print \"Vectorización:\", vector_tiempos[2] - vector_tiempos[1]\n",
    "print \"Hashing trick:\", vector_tiempos[3] - vector_tiempos[2]\n",
    "\n",
    "#testing = sc.parallelize(reviews.take(10), 4)\n",
    "#print \"Antes:\", testing.collect()\n",
    "#testing = predecir_SVM(testing)\n",
    "#print \"Después\", testing.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fooCount(x):\n",
    "    global contadora\n",
    "    contadora += 1\n",
    "    return (x, contadora)\n",
    "\n",
    "print reviews.take(5)\n",
    "print reviews.map(lambda x: (np.array_str(x[0]), x[1]) ).take(5)\n",
    "\n",
    "np.array_str(reviews.take(1)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "382"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = data.map(lambda x: (x.Text, x.Prediction))\n",
    "reviews.filter(lambda x: x[1] == 5).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "322"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.filter(lambda x: x[1] == 5).sample(False, 0.82).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
