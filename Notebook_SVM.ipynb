{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lectura del archivo .csv de training\n",
    "import pyspark_csv as pycsv\n",
    "sc.addPyFile('pyspark_csv.py')\n",
    "plaintext_rdd = sc.textFile('train_train.csv')\n",
    "dataframe = pycsv.csvToDataFrame(sqlCtx, plaintext_rdd, parseDate=False)\n",
    "\n",
    "\n",
    "data = dataframe.rdd\n",
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup # Para eliminar tags html\n",
    "import re # Expresiones regulares para eliminar puntuacion\n",
    "from nltk.corpus import stopwords # Stopwords para eliminar palabras comunes\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "def aplicarStemming(x):\n",
    "    words = x.split()\n",
    "    st = LancasterStemmer()\n",
    "    new_words = []\n",
    "    for w in words:\n",
    "        new_words.append(st.stem(w))\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "def borrarPalabrasComunes(x):\n",
    "    words = x.split()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    stop_words.remove(\"not\")\n",
    "    stop_words.remove(\"hadn\")\n",
    "    stop_words.remove(\"hasn\")\n",
    "    stop_words.remove(\"didn\")\n",
    "    new_words = []\n",
    "    for w in words:\n",
    "        if(not w in stop_words):\n",
    "            new_words.append(w)\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "def eliminarMenosUsadas(x):\n",
    "    global menosUsadas\n",
    "    print menosUsadas\n",
    "    words = x.split()    \n",
    "    new_words = []\n",
    "    for w in words:\n",
    "        if(not w in menosUsadas):\n",
    "            new_words.append(w)\n",
    "    return \" \".join(new_words)\n",
    "    \n",
    "def borrarSimbolos(x):\n",
    "    aBorrar = \",@#$-.():[]!?'\"\n",
    "    for c in aBorrar:\n",
    "        x = x.replace(c, \"\")\n",
    "    return x\n",
    "\n",
    "def explicitarNegacion(x):\n",
    "    negadores = [\"not\", \"no\", \"dont\", \"doesnt\", \"havent\", \"hasnt\", \"isnt\", \"arent\"\n",
    "                \"wont\", \"aint\", \"didnt\", \"hadnt\"]\n",
    "    words = x.split()\n",
    "    new_words = []\n",
    "    i = 0\n",
    "    while(i < len(words)):\n",
    "        if(words[i] in negadores):\n",
    "            new_words.append(\"NOT_\"+words[i+1].upper())\n",
    "            i += 1\n",
    "        else:\n",
    "            new_words.append(words[i])\n",
    "        i += 1\n",
    "    return \" \".join(new_words)\n",
    "\n",
    "# El formato de n-gramas es el siguiente: los n-gramas se devuelven en un\n",
    "# string, separados cada uno por un espacio. Para considerar los espacios\n",
    "# \"verdaderos\" del texto original, se los reemplaza primero por @.\n",
    "# Por ejemplo, el string \"Un buen dia\" se traduce con esta función, y tam.\n",
    "# de n-grama=3 al string \"Un@ n@b @bu bue uen en@ n@d @di dia\".\n",
    "def conseguirNgramas(x):\n",
    "    ngramSize = 3\n",
    "    old_string = x.replace(\" \", \"@\")\n",
    "    if(len(old_string) < ngramSize):\n",
    "        return old_string\n",
    "    new_string = old_string[0:ngramSize]\n",
    "    for i in range(1, len(old_string)-ngramSize+1):\n",
    "        new_string += \" \"\n",
    "        new_string += old_string[i:(i+ngramSize)]\n",
    "    return new_string\n",
    "\n",
    "def considerarEmoticonesPuntuacion(x):\n",
    "    # Lista de caritas felices\n",
    "    caras_felices = [\":)\", \"(:\", \"[:\", \":]\", \"c:\", \"=)\", \"=]\", \"(=\", \"[=\", \"c=\",\n",
    "                    \"=D\", \":D\", \";)\", \"(;\", \";D\"]\n",
    "    for emoji in caras_felices:\n",
    "        x = x.replace(emoji , \"SMILING_FACE\")\n",
    "    # Lista de caritas tristes\n",
    "    caras_tristes = [\":(\", \":[\", \"):\", \"]:\", \":c\", \"=(\", \"=[\", \"]=\", \"=c\", \"D=\", \n",
    "                    \"D:\", \";(\", \");\", \"D;\", ]\n",
    "    for emoji in caras_tristes:\n",
    "        x = x.replace(emoji, \"SAD_FACE\")\n",
    "    # Lista de caritas sorprendidas\n",
    "    caras_sorpr = [\":0\", \":o\", \"0:\", \"o:\", \"=o\", \"0=\"]\n",
    "    for emoji in caras_sorpr:\n",
    "        x = x.replace(emoji, \"SURPRISED_FACE\") \n",
    "    # Puntuación (signos ! y ?)\n",
    "    x = x.replace(\"!!!\", \" ADMIRx3\")\n",
    "    x = x.replace(\"!!\", \" ADMIRx2\")\n",
    "    x = x.replace(\"???\", \" QUESx3\")\n",
    "    x = x.replace(\"??\", \" QUESx2\")\n",
    "    x = x.replace(\"?!\", \" ADM_QUES\")\n",
    "    x = x.replace(\"!?\", \" ADM_QUES\")\n",
    "    x = x.replace(\"!\", \" ADMIRx1\")\n",
    "    x = x.replace(\"?\", \" QUESx1\")\n",
    "    return x\n",
    "\n",
    "# Función encargada de realizar un pre-procesamiento de los textos de las reviews\n",
    "# según lo considerado por nuestro diseño del TP. Para ello, se recibe el set de\n",
    "# entrenamiento como un RDD de reviews, que son tuplas (texto, puntaje).\n",
    "# Las distintas acciones que la función realiza sobre el texto de las reviews\n",
    "# dependen de los flags de procesamiento recibidos en flagsP (como lista).\n",
    "# A continuación la lista de acciones controlada por cada flag de flagsP:\n",
    "# flagsP[0] controla la eliminación de palabras comunes (\"a\", \"the\", \"of\", etc.)\n",
    "# fragsP[1] elimina las palabras de frecuencia menor a *elMinimo*\n",
    "# flagsP[2] activa el uso de stemming sobre las palabras de la review\n",
    "# flagsP[3] activa el reconocimiento de emoticones y puntuaciones ?,!\n",
    "# flagsP[4] activa la explicitación de la negación\n",
    "# flagsP[5] convierte el texto a n-gramas\n",
    "# Notar que las acciones se hacen en el orden explicitado por los flags (primero se eliminan\n",
    "# las palabras comunes, después las de frecuencia menor, después stemming, etc.)\n",
    "# Acciones que el pre-procesador de reviews hace siempre:\n",
    "# - Eliminar tags html\n",
    "# - Convertir todo a minúsculas\n",
    "# - Eliminar los siguientes símbolos: \",\" \"@\" \"#\" \"$\" \"-\" \".\" \"(\" \")\" \":\" \"]\" \"[\"\n",
    "# (En el caso de considerar emoticones o puntuación no lo hace hasta después de\n",
    "# detectar todos los emoticones o símbolos deseados correspondientes)\n",
    "def preprocesar_reviews(elSet, flagsP):\n",
    "    nuevoSet = elSet.map(lambda x: (BeautifulSoup(x[0], \"lxml\").getText(), x[1]) )\n",
    "    nuevoSet = nuevoSet = nuevoSet.map(lambda x: (x[0].lower(), x[1]))\n",
    "    \n",
    "    if(flagsP[0]):\n",
    "        nuevoSet = nuevoSet.map(lambda x: (borrarPalabrasComunes(x[0]), x[1]))\n",
    "        \n",
    "    if(flagsP[1]):\n",
    "        global menosUsadas\n",
    "        elMinimo = 10\n",
    "        setFrec = nuevoSet.flatMap(lambda x: x[0].split()).map(lambda x: (x, 1))\n",
    "        setFrec = setFrec.reduceByKey(lambda x,y: x+y)\n",
    "        menosUsadas = setFrec.filter(lambda x: x[1] < elMinimo).map(lambda x: x[0]).collect()\n",
    "        nuevoSet = nuevoSet.map(lambda x: (eliminarMenosUsadas(x[0]), x[1]))\n",
    "    \n",
    "    if(flagsP[2]):\n",
    "        nuevoSet = nuevoSet.map(lambda x: (aplicarStemming(x[0]), x[1]))\n",
    "    \n",
    "    if(flagsP[3]):\n",
    "        nuevoSet = nuevoSet.map(lambda x: (considerarEmoticonesPuntuacion(x[0]), x[1]))\n",
    "    nuevoSet = nuevoSet = nuevoSet.map(lambda x: (borrarSimbolos(x[0]), x[1]))\n",
    "    \n",
    "    if(flagsP[4]):\n",
    "        nuevoSet = nuevoSet.map(lambda x: (explicitarNegacion(x[0]), x[1]))\n",
    "    \n",
    "    if(flagsP[5]):\n",
    "        nuevoSet = nuevoSet.map(lambda x: (conseguirNgramas(x[0]), x[1]))\n",
    "    \n",
    "    return nuevoSet\n",
    "\n",
    "menosUsadas = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def parsearReview(x):\n",
    "    texto = x[0]\n",
    "    tabla = x[1]\n",
    "    palabras = texto.split()\n",
    "    for w in palabras:\n",
    "        tabla.aumentarFrecuencia(w)\n",
    "    return tabla\n",
    "\n",
    "# Método auxiliar usado en el map-reduce: Fusiona dos diccionarios\n",
    "def unirDiccionarios(dicA, dicB):\n",
    "    # Copio A en C\n",
    "    dicC = {}\n",
    "    for clave in dicA.keys():\n",
    "        valor = dicA[clave]\n",
    "        dicC[clave] = valor\n",
    "    # Veo ahora los simbolos en B\n",
    "    for clave in dicB.keys():\n",
    "        valor = dicB[clave]\n",
    "        if dicC.has_key(clave):\n",
    "            dicC[clave] += valor\n",
    "        else:\n",
    "            dicC[clave] = valor\n",
    "    return dicC\n",
    "\n",
    "# Método auxiliar de vectorización: Genera un numpy.vector que representa\n",
    "# la cant. de palabras de un determinado review.\n",
    "def vectorizar(x):\n",
    "    global tabla_vec\n",
    "    words = tabla_vec.keys()\n",
    "    p = []\n",
    "    for w in words:\n",
    "        if(x.has_key(w)):\n",
    "            p.append(float(x[w]))\n",
    "        else:\n",
    "            p.append(0.0)\n",
    "    return np.array(p)\n",
    "\n",
    "# vectorizar_reviews es la función encargada de convertir en numpy.vector\n",
    "# todos los textos de todas las reviews. Para ello, recibe como parámetro\n",
    "# el set de entrenamiento, que debe tener el sge formato: debe ser un RDD\n",
    "# formado por tuplas (texto, puntaje) de cada review. El método devuelve\n",
    "# otro RDD con el formato (vector, puntaje) donde vector es un numpy.vector\n",
    "# que representa el texto de cada review recibida. Cada vector se consigue\n",
    "# de la sgte forma: Primero se listan todas las palabras de todas las\n",
    "# reviews del set, y se les asigna a cada una de ellas un índice del vector; \n",
    "# luego, para cada review del set, se cuentan cuántas palabras hay y cuáles,\n",
    "# y se ponen esos valores de contadores en las posiciones correspondientes\n",
    "# del vector. Por ejemplo, si todas las palabras son [\"Casa\", \"Pez\", \"Arbol\"]\n",
    "# una review de la forma \"Casa Pez Casa\" se traducirá como el vector (2, 1, 0)\n",
    "# NOTA: Si se quiere que en vez de contar las palabras se cargue sólamente un\n",
    "# 1 o un 0 según si la palabra está presente o no, se debe cambiar en la\n",
    "# función vectorizar de arriba el p.append(float(x[w])) por un p.append(1.0)\n",
    "# ADVERTENCIA: El vectorizador no hace ningún procesamiento del texto, y\n",
    "# separa sólo las palabras por espacios. Cualquier pre-procesamiento (por\n",
    "# ejemplo, eliminar las \",\") debe hacerse antes de llamarlo.\n",
    "def vectorizar_reviews(elSet):\n",
    "    # Primero consigo en tabla_vec un diccionario con todas las palabras\n",
    "    # de todas las reviews del set\n",
    "    global tabla_vec\n",
    "    tabla_vec = elSet.map(lambda x: (x[0], SymbolTable())).map(parsearReview)\n",
    "    tabla_vec = tabla_vec.map(lambda x: dict(x.verItems()))\n",
    "    tabla_vec = tabla_vec.reduce(unirDiccionarios)\n",
    "    # Vectorizo ahora todas las reviews del set usando esa tabla\n",
    "    reviews_vec = reviews.map(lambda x: (x[0], SymbolTable(), x[1]))\n",
    "    reviews_vec = reviews_vec.map(lambda x: (parsearReview(x), x[2]))\n",
    "    reviews_vec = reviews_vec.map(lambda x: (dict(x[0].verItems()), x[1]))\n",
    "    reviews_vec = reviews_vec.map(lambda x: (vectorizar(x[0]), x[1]))\n",
    "    return reviews_vec\n",
    "\n",
    "\n",
    "tabla_vec = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Todo este bloque define la realización del k-fold crossed validation.\n",
    "# El método funciona así: recibe un set de entrenamiento y hace sobre el\n",
    "# mismo la técnica de k-fold crossed validation. El formato del set debe\n",
    "# ser un RDD de TUPLAS de la forma: (features, categoria) donde la clave\n",
    "# features puede ser cualquier basura, y categoria es el valor numérico\n",
    "# que se desea predecir (aka el puntaje de cada review). En cada pasada\n",
    "# del k-fold crossed validation, se invocan a las funciones de entrenar\n",
    "# func_entrenar y a las de predicción func_predecir. Éstas dos funciones\n",
    "# deben trabajar de manera global con el/los compresor/es o el SVM. Sus\n",
    "# firmas deben ser las siguientes:\n",
    "# func_entrenar recibe un set de entrenamiento (en el mismo formato que\n",
    "# el set original, como tuplas feature,cat.) y prepara al compresor o SVM\n",
    "# para las predicciones usando ese set.\n",
    "# func_predecir recibe un set a predecir (en el mismo formato de tuplas\n",
    "# feature, cat) y debe devolver OTRO set (también en el mismo formato!)\n",
    "# que correspondan a las predicciones hechas por el SVM o compresores.\n",
    "# Observación importante: como el k-fold crossed validation en sí no\n",
    "# tiene ni idea qué usamos para predecir, todo lo demás ajeno a eso,\n",
    "# incluyendo la selección de hiperparametros, debe hacerse \"por fuera\",\n",
    "# ya sea con un pre-procesamiento de las reviews o en la función de entrenar.\n",
    "def fooCount(x):\n",
    "    global contadora\n",
    "    contadora += 1\n",
    "    return (x, contadora)\n",
    "\n",
    "def calculo_ECM(predSet, valSet):\n",
    "    cant = predSet.count()\n",
    "    setAux = predSet.union(valSet)\n",
    "    setAux = setAux.map(lambda x: (np.array_str(x[0]), x[1]) )\n",
    "    setAux = setAux.reduceByKey(lambda x,y: float(x)-float(y)).map(lambda x: x[1]*x[1])\n",
    "    ecm = setAux.reduce(lambda x,y: x+y)\n",
    "    return (ecm/float(cant))\n",
    "\n",
    "def k_fold_crossed_validation(elSet, func_entrenar, func_predecir):\n",
    "    cantParticiones = 8\n",
    "    ecm_acum = 0.0\n",
    "    largoSet = elSet.count()\n",
    "    largoParticion = largoSet / cantParticiones\n",
    "    setauxi = elSet.map(fooCount)\n",
    "    for j in range (1, cantParticiones+1):\n",
    "        # Obtengo el testSet como la particion j-ésima y el trainSet como\n",
    "        # todo el resto del set recibido menos el testSet\n",
    "        #print \"El set:\", elSet.take(2)\n",
    "        testSet = setauxi.filter(lambda x: (x[1] % cantParticiones) == (j-1)).map(lambda x: x[0])\n",
    "        trainSet = setauxi.filter(lambda x: (x[1] % cantParticiones) == (j-1)).map(lambda x: x[0])\n",
    "        #print \"Antes de entrenar:\", trainSet.take(2)\n",
    "        # Entreno contra trainSet\n",
    "        func_entrenar(trainSet)\n",
    "        # Testeo contra testSet\n",
    "        setResultados = func_predecir(testSet)\n",
    "        ecm_acum += calculo_ECM(setResultados, testSet)\n",
    "        print \"ECM acumulado iteracion\", j, \"es:\", ecm_acum\n",
    "    # Obtengo el ECM promedio de la validación\n",
    "    print \"ECM promedio:\", (ecm_acum/float(cantParticiones))\n",
    "    \n",
    "    \n",
    "contadora = 0 # Se usa, no tocar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from symbol_table import *\n",
    "from MulticatSVM import *\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold='nan')\n",
    "\n",
    "def funcion_hash(numero):\n",
    "    dim_k = 25\n",
    "    return (numero % dim_k)\n",
    "\n",
    "def hashing_trick(x):\n",
    "    dim_k = 25\n",
    "    vec_k = []\n",
    "    for i in range(0, dim_k):\n",
    "        vec_k.append(0.0)\n",
    "    vec_k = np.array(vec_k)\n",
    "    for i in range(0, x.size):\n",
    "        vec_k[funcion_hash(i)] += x[i]\n",
    "    return vec_k\n",
    "\n",
    "def entrenar_SVM(trainSet):\n",
    "    global our_svm\n",
    "    our_svm = MulticatSVM(dim = dim_datos, cte_soft_margin = C_inicial, cantCategorias = 5)\n",
    "    # Primero necesito separar las reviews de acuerdo a su puntaje\n",
    "    rev_por_puntaje = []\n",
    "    for i in range(1, 6): # 5 puntajes posibles\n",
    "        rev_act = trainSet.filter(lambda x: x[1] == i).map(lambda x: x[0]).collect()\n",
    "        rev_por_puntaje.append(rev_act)\n",
    "    # Tengo en rev_por_puntaje los datos en el formato que el MulticatSVM\n",
    "    # los necesita => Lo entreno!\n",
    "    our_svm.entrenar(rev_por_puntaje)\n",
    "    return\n",
    "\n",
    "def aplicar_prediccion(x):\n",
    "    global our_svm\n",
    "    return our_svm.predecir(x)\n",
    "\n",
    "def predecir_SVM(testSet):\n",
    "    global our_svm\n",
    "    setResultados = testSet.map(lambda x: (x[0], aplicar_prediccion(x[0])) )\n",
    "    return setResultados\n",
    "\n",
    "# Primero guardo las reviews en formato (texto, puntaje) y pre-proceso\n",
    "reviews = data.map(lambda x: (x.Text, x.Prediction))\n",
    "reviews = preprocesar_reviews(reviews, [1, 1, 1, 0, 0, 0])\n",
    "# Vectorizo el set:\n",
    "tabla_vec = {} # Importante: Esta tabla vec se va a usar\n",
    "reviews = vectorizar_reviews(reviews)\n",
    "# Aplico hashing trick!\n",
    "reviews = reviews.map(lambda x: (hashing_trick(x[0]), x[1]) )\n",
    "# Hagamos un SVM multicategoría\n",
    "dim_datos = reviews.take(5)[0][0].size\n",
    "C_inicial = 9000.0 #cte inicial soft-margin\n",
    "our_svm = MulticatSVM(dim = dim_datos, cte_soft_margin = C_inicial, cantCategorias = 5)\n",
    "# Hago k-fold crossed validation contra las reviews\n",
    "k_fold_crossed_validation(reviews, entrenar_SVM, predecir_SVM)\n",
    "#entrenar_SVM(reviews)\n",
    "\n",
    "#testing = sc.parallelize(reviews.take(10), 4)\n",
    "#print \"Antes:\", testing.collect()\n",
    "#testing = predecir_SVM(testing)\n",
    "#print \"Después\", testing.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fooCount(x):\n",
    "    global contadora\n",
    "    contadora += 1\n",
    "    return (x, contadora)\n",
    "\n",
    "print reviews.take(5)\n",
    "print reviews.map(lambda x: (np.array_str(x[0]), x[1]) ).take(5)\n",
    "\n",
    "np.array_str(reviews.take(1)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
